{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/profliuhao/CSIT599/blob/main/CSIT599_module5_exercise1_seq2seq_attention_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Exercise 1: Sequence-to-Sequence Models with and without Attention\n",
        "\n",
        "Student Name: ____________________\n",
        "\n",
        "## English to German Translation\n",
        "\n",
        "Learning Objectives:\n",
        "1. Build encoder-decoder architecture using LSTM (WITHOUT attention)\n",
        "2. Build encoder-decoder architecture using LSTM (WITH attention)\n",
        "3. Compare performance using Loss, Accuracy, and BLEU score\n",
        "\n",
        "Instructions:\n",
        "- Fill in the blanks marked with \\_\\_\\_BLANK___\n",
        "- The exercise is divided into TWO SEPARATE PARTS\n",
        "- Part A: Model WITHOUT Attention\n",
        "- Part B: Model WITH Attention\n",
        "- Part C: Comparison with metrics\n",
        "- Each blank is a simple parameter, function name, or dimension\n",
        "- Run the code to see the comparison results\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l6r-z6gXVUe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Set random seeds for reproducibility across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYo18db3Vl0u",
        "outputId": "77bce9d5-57fb-4cb7-ea57-fe18c85163b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "GPU Available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# HYPERPARAMETERS: These are constant values that configure the model and training process.\n",
        "# ==============================================================================\n",
        "\n",
        "BATCH_SIZE = 64        # Number of samples processed before the model is updated\n",
        "EMBEDDING_DIM = 512    # Dimension of the dense embedding for each word\n",
        "LSTM_UNITS = 256       # Number of hidden units in the LSTM layers. This determines the capacity of the model.\n",
        "EPOCHS = 20            # Number of complete passes through the training dataset\n",
        "MAX_VOCAB_SIZE = 10000 # Maximum number of unique words to consider for the vocabulary\n",
        "MAX_LENGTH = 20        # Maximum sequence length for both input and target sentences. Sentences longer than this will be truncated, shorter ones will be padded.\n"
      ],
      "metadata": {
        "id": "qL-aWWOhVp-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PART 0: DATA LOADING AND PREPROCESSING (SHARED BY BOTH MODELS)\n",
        "# This section handles downloading, cleaning, and structuring the raw text data.\n",
        "# ==============================================================================\n",
        "\n",
        "def download_data():\n",
        "    \"\"\"Download the English-German translation dataset from a specified URL.\"\"\"\n",
        "    url = \"http://www.manythings.org/anki/deu-eng.zip\"\n",
        "    filename = \"deu-eng.zip\"\n",
        "\n",
        "    # Check if the unzipped data file 'deu.txt' already exists to avoid re-downloading\n",
        "    if not os.path.exists(\"deu.txt\"):\n",
        "        print(\"Downloading dataset...\")\n",
        "        # Add User-Agent header to avoid 406 Not Acceptable error from some servers\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "        req = urllib.request.Request(url, headers=headers)\n",
        "        with urllib.request.urlopen(req) as response, open(filename, 'wb') as out_file:\n",
        "            out_file.write(response.read()) # Read data from the URL and write to a local file\n",
        "\n",
        "        # Unzip the downloaded file\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall() # Extract all contents to the current directory\n",
        "        os.remove(filename) # Remove the zip file after extraction to save space\n",
        "        print(\"Download complete!\")\n",
        "    else:\n",
        "        print(\"Dataset already exists.\")\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    \"\"\"Lowercase the sentence, add spaces around punctuation, and add <start>/<end> tokens.\"\"\"\n",
        "    sentence = sentence.lower().strip() # Convert to lowercase and remove leading/trailing whitespace\n",
        "    # Add space between words and punctuation to ensure tokenization treats punctuation as separate tokens\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # Replace multiple spaces with a single space\n",
        "    sentence = sentence.strip() # Strip again after adding spaces for cleanliness\n",
        "    # Add special tokens to mark the beginning and end of a sentence, crucial for sequence models\n",
        "    sentence = '<start> ' + sentence + ' <end>'\n",
        "    return sentence\n",
        "\n",
        "def load_dataset(num_examples=10000):\n",
        "    \"\"\"Load and preprocess the dataset from 'deu.txt'.\"\"\"\n",
        "    download_data() # Ensure the dataset is downloaded\n",
        "\n",
        "    # Read the file content\n",
        "    with open('deu.txt', 'r', encoding='utf-8') as f:\n",
        "        lines = f.read().strip().split('\\n') # Read all lines and split them into a list\n",
        "\n",
        "    # Parse English-German pairs\n",
        "    pairs = []\n",
        "    for line in lines[:num_examples]: # Iterate through a specified number of examples\n",
        "        parts = line.split('\\t') # Split each line by tab character (English and German are tab-separated)\n",
        "        if len(parts) >= 2:\n",
        "            eng = preprocess_sentence(parts[0]) # Preprocess English sentence\n",
        "            deu = preprocess_sentence(parts[1]) # Preprocess German sentence\n",
        "            # Filter sentences by maximum length to keep training manageable and consistent\n",
        "            if len(eng.split()) <= MAX_LENGTH and len(deu.split()) <= MAX_LENGTH:\n",
        "                pairs.append([eng, deu]) # Add valid pairs to the list\n",
        "\n",
        "    print(f\"Loaded {len(pairs)} sentence pairs\")\n",
        "    return zip(*pairs)  # Returns two separate iterators: (english_sentences, german_sentences)\n",
        "\n",
        "# Load data into input_texts (English) and target_texts (German)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "input_texts, target_texts = load_dataset(num_examples=20000) # Load 20,000 sentence pairs\n",
        "input_texts = list(input_texts) # Convert iterators to lists for easier manipulation\n",
        "target_texts = list(target_texts)\n",
        "\n",
        "print(f\"\\nExample sentence pair:\")\n",
        "print(f\"English: {input_texts[0]}\")\n",
        "print(f\"German:  {target_texts[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkudkQCNVucF",
        "outputId": "6f9211d8-ca12-4ab3-abed-a89e9b23a5a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "LOADING DATA\n",
            "======================================================================\n",
            "Dataset already exists.\n",
            "Loaded 20000 sentence pairs\n",
            "\n",
            "Example sentence pair:\n",
            "English: <start> go . <end>\n",
            "German:  <start> geh . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# TOKENIZATION: Converting text sentences into numerical sequences for model input.\n",
        "# ==============================================================================\n",
        "\n",
        "# Create tokenizers for English (input) and German (target) languages\n",
        "input_tokenizer = keras.preprocessing.text.Tokenizer(\n",
        "    num_words=MAX_VOCAB_SIZE, # Limit the vocabulary size to the most frequent words, improving efficiency and reducing noise.\n",
        "    filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',  # Characters to filter out. Keeping '<' and '>' for special tokens like <start> and <end>.\n",
        "    oov_token='<UNK>' # Token for out-of-vocabulary words. Words not in the top MAX_VOCAB_SIZE will be replaced with this.\n",
        ")\n",
        "\n",
        "target_tokenizer = keras.preprocessing.text.Tokenizer(\n",
        "    num_words=MAX_VOCAB_SIZE, # Limit the vocabulary size for target language to manage complexity.\n",
        "    filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',\n",
        "    oov_token='<UNK>'\n",
        ")\n",
        "\n",
        "# Fit tokenizers on the data to build the vocabulary index (mapping words to integers)\n",
        "input_tokenizer.fit_on_texts(input_texts)\n",
        "target_tokenizer.fit_on_texts(target_texts)\n",
        "\n",
        "# Convert text sentences to sequences of integer tokens. Each word is replaced by its corresponding integer ID.\n",
        "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
        "target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n",
        "\n",
        "# BLANK: What type of padding should we use? ('pre' or 'post')\n",
        "# Hint: We typically use 'post' padding for seq2seq models. This means adding zeros at the end of the sequences.\n",
        "# This is important for LSTM layers to process the sequence correctly from start to end.\n",
        "input_sequences = keras.preprocessing.sequence.pad_sequences(\n",
        "    input_sequences,\n",
        "    maxlen=MAX_LENGTH, # Pad or truncate sequences to this maximum length, ensuring all sequences have the same dimension.\n",
        "    padding=___BLANK_______   # Padding with zeros at the end of the sequence. This is standard for encoder-decoder models.\n",
        ")\n",
        "\n",
        "target_sequences = keras.preprocessing.sequence.pad_sequences(\n",
        "    target_sequences,\n",
        "    maxlen=MAX_LENGTH,\n",
        "    padding=___BLANK_______   # Same as above, padding targets with zeros at the end to maintain consistent input shapes for the model.\n",
        ")\n",
        "\n",
        "# Vocabulary sizes: +1 for the 0-th index used for padding. The 0 index is reserved for padding, so actual words start from 1.\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1 # Total unique words in the input vocabulary plus padding index.\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1 # Total unique words in the target vocabulary plus padding index.\n",
        "\n",
        "print(f\"\\nVocabulary sizes:\")\n",
        "print(f\"English: {input_vocab_size}\")\n",
        "print(f\"German: {target_vocab_size}\")\n",
        "print(f\"Input shape: {input_sequences.shape}\") # Expected shape: (number_of_samples, MAX_LENGTH)\n",
        "print(f\"Target shape: {target_sequences.shape}\") # Expected shape: (number_of_samples, MAX_LENGTH)\n",
        "\n",
        "# Split data into training and validation sets (90/10 split).\n",
        "split_idx = int(0.9 * len(input_sequences))\n",
        "train_input = input_sequences[:split_idx] # 90% for training\n",
        "train_target = target_sequences[:split_idx]\n",
        "val_input = input_sequences[split_idx:]     # Remaining 10% for validation\n",
        "val_target = target_sequences[split_idx:]\n",
        "\n",
        "# Keep the original text for BLEU score calculation, as BLEU operates on raw text.\n",
        "# This allows for a direct comparison with human-readable translations.\n",
        "train_input_texts = input_texts[:split_idx]\n",
        "train_target_texts = target_texts[:split_idx]\n",
        "val_input_texts = input_texts[split_idx:]\n",
        "val_target_texts = target_texts[split_idx:]\n",
        "\n",
        "print(f\"\\nTraining samples: {len(train_input)}\")\n",
        "print(f\"Validation samples: {len(val_input)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MPBzMDRV04p",
        "outputId": "d02d400f-aaa2-4a50-fac2-c764e7aae191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulary sizes:\n",
            "English: 3528\n",
            "German: 5676\n",
            "Input shape: (20000, 20)\n",
            "Target shape: (20000, 20)\n",
            "\n",
            "Training samples: 18000\n",
            "Validation samples: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# EVALUATION METRICS: Functions to assess the performance of the translation models.\n",
        "# ==============================================================================\n",
        "\n",
        "def calculate_accuracy(predictions, targets):\n",
        "    \"\"\"\n",
        "    Calculate token-level accuracy, ignoring padding tokens. This measures how many predicted words match the ground truth, excluding padding zeros.\n",
        "\n",
        "    Args:\n",
        "        predictions: Tensor of shape [batch_size, seq_len, vocab_size]. These are the raw output logits from the decoder for each time step.\n",
        "        targets: Tensor of shape [batch_size, seq_len]. These are the ground truth target sequences (integer token IDs).\n",
        "\n",
        "    Returns:\n",
        "        accuracy: float, the average token accuracy over the batch, ignoring padding tokens.\n",
        "    \"\"\"\n",
        "    # Get predicted token IDs by taking the argmax along the vocabulary dimension.\n",
        "    # This selects the token with the highest probability/logit for each time step in the output sequence.\n",
        "    # BLANK: Which axis should we take argmax over to get predicted tokens?\n",
        "    # Hint: We want the highest scoring token in the vocabulary (last dimension).\n",
        "    predicted_ids = tf.argmax(predictions, axis=___BLANK_______)  # -1 refers to the last dimension (vocabulary size) to get the most probable token ID.\n",
        "\n",
        "    # Create a mask to identify non-padding tokens. Padding tokens are typically represented by 0 and should not contribute to accuracy.\n",
        "    mask = tf.math.not_equal(targets, 0) # Returns a boolean tensor where True indicates a non-padding token.\n",
        "\n",
        "    # Compare the predicted token IDs with the actual target token IDs.\n",
        "    # `tf.equal` returns a boolean tensor where True indicates a match between prediction and target.\n",
        "    matches = tf.equal(predicted_ids, targets)\n",
        "\n",
        "    # Apply the mask: only consider matches for non-padding tokens. `tf.boolean_mask` filters out values where the mask is False.\n",
        "    matches = tf.boolean_mask(matches, mask)\n",
        "\n",
        "    # Calculate the mean of the boolean matches (True=1, False=0) to get the accuracy.\n",
        "    accuracy = tf.reduce_mean(tf.cast(matches, tf.float32)) # Cast boolean to float (True=1.0, False=0.0) and compute the average.\n",
        "\n",
        "    return accuracy.numpy() # Convert TensorFlow tensor to a NumPy float for easier handling.\n",
        "\n",
        "def calculate_bleu_score(reference, candidate):\n",
        "    \"\"\"\n",
        "    Calculate BLEU score (simple implementation). BLEU (Bilingual Evaluation Understudy) measures how similar the machine translation is to human translation.\n",
        "    Score ranges from 0 (worst) to 1 (perfect match).\n",
        "\n",
        "    Args:\n",
        "        reference: A string representing the reference translation (ground truth, human-generated).\n",
        "        candidate: A string representing the model's generated translation.\n",
        "\n",
        "    Returns:\n",
        "        bleu_score: float, the calculated BLEU score.\n",
        "    \"\"\"\n",
        "    # Tokenize the reference and candidate sentences into lists of words.\n",
        "    reference_tokens = reference.lower().split()\n",
        "    candidate_tokens = candidate.lower().split()\n",
        "\n",
        "    # Remove special start and end tokens as they are not part of the actual translation content and shouldn't affect BLEU.\n",
        "    reference_tokens = [t for t in reference_tokens if t not in ['<start>', '<end>']]\n",
        "    candidate_tokens = [t for t in candidate_tokens if t not in ['<start>', '<end>']]\n",
        "\n",
        "    if len(candidate_tokens) == 0:\n",
        "        return 0.0 # Return 0 if the candidate translation is empty to avoid division by zero.\n",
        "\n",
        "    # Count word occurrences for both reference and candidate to calculate precision.\n",
        "    reference_counts = Counter(reference_tokens) # Dictionary-like object to count token frequencies\n",
        "    candidate_counts = Counter(candidate_tokens)\n",
        "\n",
        "    # Calculate the number of 'clipped' matches: sum of minimum counts for each token\n",
        "    # present in both candidate and reference. This prevents over-counting common words and giving unfair high precision.\n",
        "    matches = sum((min(candidate_counts[token], reference_counts[token])\n",
        "                   for token in candidate_counts)) # Sum of minimum counts for tokens present in candidate.\n",
        "\n",
        "    # Calculate precision: ratio of matched tokens to the total number of tokens in the candidate translation.\n",
        "    precision = matches / len(candidate_tokens)  # Divide by the total number of candidate tokens to get the proportion of correctly translated words.\n",
        "\n",
        "    # Apply a brevity penalty if the candidate translation is shorter than the reference.\n",
        "    # This penalizes models that generate very short, high-precision translations which might miss information.\n",
        "    if len(candidate_tokens) < len(reference_tokens):\n",
        "        brevity_penalty = np.exp(1 - len(reference_tokens) / len(candidate_tokens)) # Exponential penalty for brevity\n",
        "    else:\n",
        "        brevity_penalty = 1.0 # No penalty if candidate is equal or longer than reference.\n",
        "\n",
        "    # The final BLEU score (simplified for this exercise) is the brevity penalty multiplied by precision.\n",
        "    # A full BLEU score uses geometric mean of n-gram precisions and a more robust brevity penalty.\n",
        "    bleu_score = brevity_penalty * precision\n",
        "\n",
        "    return bleu_score\n"
      ],
      "metadata": {
        "id": "UrhIQkgjV-Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## PART A: SEQ2SEQ MODEL WITHOUT ATTENTION\n"
      ],
      "metadata": {
        "id": "2KDtnL-mWFb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A.1: ENCODER (for model WITHOUT attention)"
      ],
      "metadata": {
        "id": "pnsxrY4nWbMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PART A.1: ENCODER (for model WITHOUT attention)\n",
        "# This Encoder processes the input sequence and produces a context vector.\n",
        "# ==============================================================================\n",
        "\n",
        "class Encoder_NoAttention(keras.Model):\n",
        "    \"\"\"\n",
        "    Encoder using LSTM (for model WITHOUT attention).\n",
        "    This encoder processes the input sequence (e.g., English sentence)\n",
        "    and converts it into a fixed-size context vector (the final LSTM states)\n",
        "    which represents the meaning of the input sequence. This context vector is then passed to the decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
        "        super(Encoder_NoAttention, self).__init__()\n",
        "\n",
        "        # Embedding layer: Converts input token IDs into dense vectors of fixed size.\n",
        "        # input_dim: The size of the vocabulary (number of unique words).\n",
        "        # output_dim: The dimension of the dense embedding. Words with similar meanings\n",
        "        #             will have similar embedding vectors, capturing semantic relationships.\n",
        "        # mask_zero=True: Allows the embedding layer to handle padding tokens (typically 0)\n",
        "        #                 by masking them out. This prevents padding from influencing the model's learning.\n",
        "        # BLANK: What is the input dimension for the embedding layer?\n",
        "        self.embedding = layers.Embedding(\n",
        "            input_dim=_____BLANK______,  # The vocabulary size of the input language (e.g., English vocabulary size).\n",
        "            output_dim=embedding_dim,\n",
        "            mask_zero=True\n",
        "        )\n",
        "\n",
        "        # LSTM layer: Processes the embedded input sequence sequentially.\n",
        "        # lstm_units: The number of units in the LSTM cell. This also determines\n",
        "        #             the dimension of the hidden and cell states (state_h and state_c).\n",
        "        # return_sequences=True: Ensures the LSTM returns the hidden states for each\n",
        "        #                        time step in the input sequence. While not directly used by the decoder in the no-attention model,\n",
        "        #                        it's often kept for consistency or if attention were to be added later.\n",
        "        # return_state=True: Ensures the LSTM returns the final hidden state (state_h)\n",
        "        #                    and final cell state (state_c) after processing the entire sequence.\n",
        "        #                    These states collectively form the context vector for the decoder in the no-attention model.\n",
        "        # BLANK: Should we return sequences? (True/False)\n",
        "        # BLANK: Should we return state? (True/False)\n",
        "        self.lstm = layers.LSTM(\n",
        "            lstm_units,\n",
        "            return_sequences=_____BLANK______,   # Return full sequence of outputs (hidden states at each timestep). While not used by no-attention decoder, it's a common LSTM setting.\n",
        "            return_state=_____BLANK______,       # Return the last hidden state (state_h) and cell state (state_c), which summarize the input sequence.\n",
        "            name='encoder_lstm_no_attention'\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        # Input x: batch of integer sequences (e.g., [batch_size, MAX_LENGTH]). Each integer represents a word ID.\n",
        "\n",
        "        # 1. Embed the input sequence\n",
        "        # The embedding layer converts token IDs into dense vectors.\n",
        "        # Output x: [batch_size, MAX_LENGTH, embedding_dim]\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # 2. Pass the embedded sequence through the LSTM layer\n",
        "        # encoder_outputs: Hidden states for each time step [batch_size, MAX_LENGTH, lstm_units].\n",
        "        # state_h: Final hidden state of the LSTM [batch_size, lstm_units]. This is the 'memory' of the encoder.\n",
        "        # state_c: Final cell state of the LSTM [batch_size, lstm_units]. This is also part of the encoder's memory.\n",
        "        # These final states (state_h, state_c) will be passed to the decoder as its initial states, providing context.\n",
        "        encoder_outputs, state_h, state_c = self.lstm(x)\n",
        "\n",
        "        return encoder_outputs, state_h, state_c\n"
      ],
      "metadata": {
        "id": "1abWIWCPWhfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A.2: DECODER WITHOUT ATTENTION"
      ],
      "metadata": {
        "id": "tasxr7XDWsw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder_NoAttention(keras.Model):\n",
        "    \"\"\"Basic Decoder WITHOUT attention. This decoder generates the target sequence one token at a time, conditioned on the encoder's final state.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
        "        super(Decoder_NoAttention, self).__init__()\n",
        "\n",
        "        # Embedding layer for the target language (e.g., German).\n",
        "        # Converts output token IDs (from previous decoding steps or <start> token) into dense vectors.\n",
        "        self.embedding = layers.Embedding(\n",
        "            input_dim=vocab_size, # Vocabulary size of the target language (e.g., German vocabulary size).\n",
        "            output_dim=embedding_dim,\n",
        "            mask_zero=True # Mask padding tokens, ensuring they don't affect embedding lookups or subsequent computations.\n",
        "        )\n",
        "\n",
        "        # LSTM layer for the decoder. It takes the embedded input token and\n",
        "        # the previous hidden state (or encoder's final state) to generate\n",
        "        # the next hidden state. It essentially learns to generate the target sequence given a context.\n",
        "        # return_sequences=True: Important for generating predictions for each time step during training.\n",
        "        #                        During inference, it would generate one prediction per step.\n",
        "        # return_state=True: Returns the final hidden and cell states, which are then fed back\n",
        "        #                    into the next time step of the LSTM during inference (when decoding one token at a time),\n",
        "        #                    or maintain state during training across the sequence.\n",
        "        self.lstm = layers.LSTM(\n",
        "            lstm_units,\n",
        "            return_sequences=True,  # Return output for each time step (essential for predicting the entire sequence during training).\n",
        "            return_state=True,      # Return the last hidden and cell state (for internal loop and next prediction in inference, or for consistency).\n",
        "            name='decoder_lstm_no_attention'\n",
        "        )\n",
        "\n",
        "        # Dense layer: Maps the LSTM's output (hidden states) to a probability distribution over the target vocabulary.\n",
        "        # The output dimension should match the vocabulary size to predict the next word.\n",
        "        # It produces logits, which are then typically passed through a softmax function (handled by the loss function).\n",
        "        # BLANK: What should be the output dimension?\n",
        "        self.dense = layers.Dense(\n",
        "            _____BLANK______,  # Output dimension should be the size of the target vocabulary to predict probabilities for every possible word.\n",
        "            name='output_dense_no_attention'\n",
        "        )\n",
        "\n",
        "    def call(self, x, initial_state):\n",
        "        # Input x: batch of integer sequences (e.g., [batch_size, MAX_LENGTH] for training, representing shifted target sequences).\n",
        "        # initial_state: A list containing [state_h, state_c] from the encoder's final states, providing the initial context.\n",
        "\n",
        "        # 1. Embed the input sequence (target tokens for current decoding step)\n",
        "        # Converts the integer IDs of the target tokens into dense vector representations.\n",
        "        # Output x: [batch_size, MAX_LENGTH, embedding_dim]\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # 2. Pass the embedded input through the LSTM with initial states from the encoder.\n",
        "        # lstm_output: Hidden states for each time step [batch_size, MAX_LENGTH, lstm_units]. These are the inputs to the dense layer.\n",
        "        # state_h, state_c: Final hidden and cell states of the decoder LSTM.\n",
        "        #                   These are used as initial states for the *next* decoding step during inference (when generating word by word).\n",
        "        lstm_output, _, _ = self.lstm(x, initial_state=initial_state)\n",
        "\n",
        "        # 3. Apply the dense layer to transform LSTM outputs into vocabulary logits.\n",
        "        # The dense layer projects the LSTM's hidden states to the vocabulary space.\n",
        "        # outputs: [batch_size, MAX_LENGTH, vocab_size] - raw logits before softmax. The values represent unnormalized log-probabilities for each word in the vocabulary.\n",
        "        outputs = self.dense(lstm_output)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "He8h6ltBWqCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A.3: BUILD MODEL WITHOUT ATTENTION"
      ],
      "metadata": {
        "id": "K59JxlvUW0NT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBuilding Seq2Seq model WITHOUT attention...\")\n",
        "\n",
        "encoder_no_attn = Encoder_NoAttention(input_vocab_size, EMBEDDING_DIM, LSTM_UNITS)\n",
        "decoder_no_attn = Decoder_NoAttention(target_vocab_size, EMBEDDING_DIM, LSTM_UNITS)\n",
        "\n",
        "print(\"✓ Encoder and Decoder (WITHOUT attention) created successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiO3BTUFW3MN",
        "outputId": "bc690122-103b-461a-8f7d-6fb93d23d62a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Building Seq2Seq model WITHOUT attention...\n",
            "✓ Encoder and Decoder (WITHOUT attention) created successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A.4: TRAINING SETUP FOR MODEL WITHOUT ATTENTION\n"
      ],
      "metadata": {
        "id": "oJ6IMOExXI5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam is a popular choice for deep learning models due to its adaptive learning rate properties.\n",
        "optimizer_no_attn = keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)  # Adam optimizer is chosen for its efficiency in handling sparse gradients and non-stationary objectives.\n",
        "\n",
        "# Loss function: SparseCategoricalCrossentropy is suitable for integer-encoded targets.\n",
        "# from_logits=True means the model outputs raw logits (unnormalized scores), and the loss function will apply softmax internally.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def train_step_no_attention(input_batch, target_batch):\n",
        "    \"\"\"Performs a single training step for the model WITHOUT attention, including forward pass, loss calculation, and backward pass (gradient update).\"\"\"\n",
        "    with tf.GradientTape() as tape: # tf.GradientTape records operations for automatic differentiation.\n",
        "        # Encoder forward pass: Process the input sequence to get encoder outputs and final states.\n",
        "        encoder_outputs, state_h, state_c = encoder_no_attn(input_batch)\n",
        "\n",
        "        # Prepare decoder input and target sequences.\n",
        "        # Decoder input is the target sequence shifted by one position to the right (starts with <start> token, ends before <end>).\n",
        "        decoder_input = target_batch[:, :-1]\n",
        "        # Decoder target is the actual target sequence shifted by one position to the left (starts after <start> token).\n",
        "        decoder_target = target_batch[:, 1:]\n",
        "\n",
        "        # Decoder forward pass: Generate predictions using the decoder, conditioned on encoder states and previous target tokens.\n",
        "        predictions = decoder_no_attn(decoder_input, [state_h, state_c])\n",
        "\n",
        "        # Calculate loss: SparseCategoricalCrossentropy compares the predicted logits with the true target token IDs.\n",
        "        # A mask is applied to ignore padding tokens (0s) in the loss calculation, so they don't penalize the model.\n",
        "        mask = tf.math.not_equal(decoder_target, 0) # Create a mask to identify non-padding tokens.\n",
        "        loss = loss_fn(decoder_target, predictions, sample_weight=mask) # Compute loss, weighted by the mask.\n",
        "\n",
        "        # Calculate accuracy: Custom accuracy function to ignore padding tokens.\n",
        "        accuracy = calculate_accuracy(predictions, decoder_target)\n",
        "\n",
        "    # Get trainable variables from both encoder and decoder.\n",
        "    trainable_vars = encoder_no_attn.trainable_variables + decoder_no_attn.trainable_variables\n",
        "\n",
        "    # Calculate gradients of the loss with respect to all trainable variables.\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "    # BLANK: What method updates the weights?\n",
        "    # The optimizer applies the calculated gradients to update the model's weights, moving towards minimizing the loss.\n",
        "    optimizer_no_attn._____BLANK______(zip(gradients, trainable_vars))  # Apply gradients to update the model's parameters.\n",
        "\n",
        "    return loss, accuracy\n",
        "\n",
        "def evaluate_no_attention(input_data, target_data):\n",
        "    \"\"\"Evaluates the model WITHOUT attention on a given dataset (e.g., validation set).\"\"\"\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    num_batches = len(input_data) // BATCH_SIZE\n",
        "\n",
        "    # Iterate through the dataset in batches for evaluation.\n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * BATCH_SIZE\n",
        "        end_idx = start_idx + BATCH_SIZE\n",
        "\n",
        "        input_batch = input_data[start_idx:end_idx]\n",
        "        target_batch = target_data[start_idx:end_idx]\n",
        "\n",
        "        # Encoder forward pass (no gradient computation needed during evaluation).\n",
        "        encoder_outputs, state_h, state_c = encoder_no_attn(input_batch)\n",
        "\n",
        "        # Prepare decoder input and target, similar to training.\n",
        "        decoder_input = target_batch[:, :-1]\n",
        "        decoder_target = target_batch[:, 1:]\n",
        "\n",
        "        # Decoder forward pass to get predictions.\n",
        "        predictions = decoder_no_attn(decoder_input, [state_h, state_c])\n",
        "\n",
        "        # Calculate loss and accuracy, ignoring padding.\n",
        "        mask = tf.math.not_equal(decoder_target, 0)\n",
        "        loss = loss_fn(decoder_target, predictions, sample_weight=mask)\n",
        "        accuracy = calculate_accuracy(predictions, decoder_target)\n",
        "\n",
        "        total_loss += loss\n",
        "        total_accuracy += accuracy\n",
        "\n",
        "    # Return average loss and accuracy over all evaluation batches.\n",
        "    return total_loss / num_batches, total_accuracy / num_batches\n"
      ],
      "metadata": {
        "id": "499FhNLOWO6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A.5: TRAINING MODEL WITHOUT ATTENTION\n"
      ],
      "metadata": {
        "id": "iVksYHaqXifb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING SEQ2SEQ WITHOUT ATTENTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "best_val_loss_no_attn = float('inf')\n",
        "best_val_acc_no_attn = 0\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    # Shuffle training data\n",
        "    indices = np.random.permutation(len(train_input))\n",
        "    train_input_shuffled = train_input[indices]\n",
        "    train_target_shuffled = train_target[indices]\n",
        "\n",
        "    # Training\n",
        "    num_batches = len(train_input) // BATCH_SIZE\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * BATCH_SIZE\n",
        "        end_idx = start_idx + BATCH_SIZE\n",
        "\n",
        "        input_batch = train_input_shuffled[start_idx:end_idx]\n",
        "        target_batch = train_target_shuffled[start_idx:end_idx]\n",
        "\n",
        "        loss, accuracy = train_step_no_attention(input_batch, target_batch)\n",
        "        total_loss += loss\n",
        "        total_accuracy += accuracy\n",
        "\n",
        "    train_loss = total_loss / num_batches\n",
        "    train_accuracy = total_accuracy / num_batches\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_accuracy = evaluate_no_attention(val_input, val_target)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{EPOCHS} - Loss: {train_loss:.4f}, Acc: {train_accuracy:.4f} | '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
        "\n",
        "    if val_loss < best_val_loss_no_attn:\n",
        "        best_val_loss_no_attn = val_loss\n",
        "        best_val_acc_no_attn = val_accuracy\n",
        "\n",
        "print(f\"\\n✓ Training complete!\")\n",
        "print(f\"  Best Validation Loss (NO ATTENTION): {best_val_loss_no_attn:.4f}\")\n",
        "print(f\"  Best Validation Accuracy (NO ATTENTION): {best_val_acc_no_attn:.4f} ({best_val_acc_no_attn*100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "b0e1ed4092614d47be3cfb356d5c73a9",
            "18e2aa25003047d0a8b0411a0255af04",
            "06794d9d6f68489496e2933d9db5de9c",
            "705b131574474875890a88f28df22dab",
            "571691118a7c4125a66c039ce2454e1e",
            "c40fa1f4bb6147abbf6eaaa62746fc0c",
            "14a712c2ca344ee198738957f9335bc5",
            "e37d51ca8793442cb903e0b5ffebf5ce",
            "0a08050cb03b4eaa8f60c814ee254c76",
            "6cdbf27ec7674cfa937b3513e67ecb49",
            "4f45736e83994f1680b686e27676b5b7"
          ]
        },
        "id": "5oORnd2AXllB",
        "outputId": "91e816f8-780b-4ed2-e51b-8251a8ef523f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "TRAINING SEQ2SEQ WITHOUT ATTENTION\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0e1ed4092614d47be3cfb356d5c73a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Loss: 3.9622, Acc: 0.3026 | Val Loss: 4.0332, Val Acc: 0.2714\n",
            "Epoch 2/20 - Loss: 3.1873, Acc: 0.3854 | Val Loss: 3.6282, Val Acc: 0.3307\n",
            "Epoch 3/20 - Loss: 2.7034, Acc: 0.4569 | Val Loss: 3.2428, Val Acc: 0.4023\n",
            "Epoch 4/20 - Loss: 2.3468, Acc: 0.5134 | Val Loss: 3.0165, Val Acc: 0.4427\n",
            "Epoch 5/20 - Loss: 2.0745, Acc: 0.5583 | Val Loss: 2.8932, Val Acc: 0.4720\n",
            "Epoch 6/20 - Loss: 1.8479, Acc: 0.5909 | Val Loss: 2.7921, Val Acc: 0.4920\n",
            "Epoch 7/20 - Loss: 1.6493, Acc: 0.6199 | Val Loss: 2.7235, Val Acc: 0.5035\n",
            "Epoch 8/20 - Loss: 1.4714, Acc: 0.6491 | Val Loss: 2.6639, Val Acc: 0.5115\n",
            "Epoch 9/20 - Loss: 1.3104, Acc: 0.6780 | Val Loss: 2.6121, Val Acc: 0.5255\n",
            "Epoch 10/20 - Loss: 1.1625, Acc: 0.7068 | Val Loss: 2.5802, Val Acc: 0.5335\n",
            "Epoch 11/20 - Loss: 1.0288, Acc: 0.7333 | Val Loss: 2.5746, Val Acc: 0.5389\n",
            "Epoch 12/20 - Loss: 0.9051, Acc: 0.7607 | Val Loss: 2.5592, Val Acc: 0.5466\n",
            "Epoch 13/20 - Loss: 0.7960, Acc: 0.7852 | Val Loss: 2.5203, Val Acc: 0.5512\n",
            "Epoch 14/20 - Loss: 0.7002, Acc: 0.8074 | Val Loss: 2.5114, Val Acc: 0.5595\n",
            "Epoch 15/20 - Loss: 0.6175, Acc: 0.8261 | Val Loss: 2.5106, Val Acc: 0.5585\n",
            "Epoch 16/20 - Loss: 0.5482, Acc: 0.8424 | Val Loss: 2.5139, Val Acc: 0.5661\n",
            "Epoch 17/20 - Loss: 0.4884, Acc: 0.8541 | Val Loss: 2.5242, Val Acc: 0.5694\n",
            "Epoch 18/20 - Loss: 0.4374, Acc: 0.8664 | Val Loss: 2.5416, Val Acc: 0.5693\n",
            "Epoch 19/20 - Loss: 0.3949, Acc: 0.8752 | Val Loss: 2.5455, Val Acc: 0.5700\n",
            "Epoch 20/20 - Loss: 0.3611, Acc: 0.8812 | Val Loss: 2.5578, Val Acc: 0.5743\n",
            "\n",
            "✓ Training complete!\n",
            "  Best Validation Loss (NO ATTENTION): 2.5106\n",
            "  Best Validation Accuracy (NO ATTENTION): 0.5585 (55.85%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART B: SEQ2SEQ MODEL WITH ATTENTION"
      ],
      "metadata": {
        "id": "ftDKqicVXsaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B.1: ENCODER (for model WITH attention)\n"
      ],
      "metadata": {
        "id": "YOH75H1AXyHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_WithAttention(keras.Model):\n",
        "    \"\"\"Encoder using LSTM (for model WITH attention). This encoder is similar to the no-attention encoder but its outputs (all hidden states) are used by the attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
        "        super(Encoder_WithAttention, self).__init__()\n",
        "\n",
        "        # Embedding layer: Converts input token IDs into dense vectors.\n",
        "        self.embedding = layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            mask_zero=True # Mask padding tokens so they don't interfere with computations.\n",
        "        )\n",
        "\n",
        "        # LSTM layer: Processes the embedded input sequence.\n",
        "        # return_sequences=True is crucial here, as the attention mechanism needs access to the hidden states at ALL timesteps of the encoder.\n",
        "        # return_state=True provides the final hidden and cell states, which are typically used as the initial state for the decoder LSTM.\n",
        "        self.lstm = layers.LSTM(\n",
        "            lstm_units,\n",
        "            return_sequences=True, # MUST return sequences for attention mechanism to work.\n",
        "            return_state=True,     # Return final hidden and cell states for decoder initialization.\n",
        "            name='encoder_lstm_with_attention'\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        # Input x: batch of integer sequences [batch_size, MAX_LENGTH]\n",
        "\n",
        "        # 1. Embed the input sequence\n",
        "        # Output x: [batch_size, MAX_LENGTH, embedding_dim]\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # 2. Pass the embedded sequence through the LSTM layer\n",
        "        # encoder_outputs: All hidden states for each time step [batch_size, MAX_LENGTH, lstm_units]. These are used by attention.\n",
        "        # state_h: Final hidden state of the LSTM [batch_size, lstm_units].\n",
        "        # state_c: Final cell state of the LSTM [batch_size, lstm_units].\n",
        "        # state_h and state_c are typically used to initialize the decoder's LSTM state.\n",
        "        encoder_outputs, state_h, state_c = self.lstm(x)\n",
        "\n",
        "        return encoder_outputs, state_h, state_c\n"
      ],
      "metadata": {
        "id": "Bz1l6WfDX0RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B.2: ATTENTION MECHANISM\n"
      ],
      "metadata": {
        "id": "AoGvtBCeX4ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(keras.layers.Layer):\n",
        "    \"\"\"Bahdanau Attention (Additive Attention). This mechanism calculates a weighted sum of encoder outputs based on the current decoder state.\n",
        "    \"\"\"\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        # W1 and W2 are dense layers used to transform the encoder outputs and decoder hidden state\n",
        "        # into a common dimension before calculating the alignment scores.\n",
        "        self.W1 = layers.Dense(units)  # The output dimension is 'units', which matches the LSTM_UNITS for consistent dimension for summation.\n",
        "        self.W2 = layers.Dense(units)\n",
        "\n",
        "        # V is a dense layer that transforms the combined energy into a single score, which is then used for softmax.\n",
        "        # BLANK: What should be the output dimension of V?\n",
        "        self.V = layers.Dense(_____BLANK______)  # The output dimension is 1, as we want a single scalar score for each encoder output.\n",
        "\n",
        "    def call(self, decoder_hidden, encoder_outputs):\n",
        "        # decoder_hidden: Current hidden state of the decoder [batch_size, lstm_units]\n",
        "        # encoder_outputs: All hidden states from the encoder [batch_size, MAX_LENGTH, lstm_units]\n",
        "\n",
        "        # We need to expand the decoder_hidden state to be able to broadcast and sum it with encoder_outputs.\n",
        "        decoder_hidden_with_time = tf.expand_dims(decoder_hidden, axis=1)  # Expand along axis 1 to get shape [batch_size, 1, lstm_units].\n",
        "\n",
        "        # Calculate the 'energy' or alignment score. This measures how well each encoder output matches the current decoder hidden state.\n",
        "        # tf.nn.tanh is used as the activation function, introducing non-linearity.\n",
        "        # BLANK: What activation function?\n",
        "        energy = tf.nn._____BLANK______(  # The hyperbolic tangent activation function is commonly used in Bahdanau attention to introduce non-linearity.\n",
        "            self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time)\n",
        "        )\n",
        "\n",
        "        # V layer then converts this energy into attention scores.\n",
        "        attention_scores = self.V(energy) # Shape: [batch_size, MAX_LENGTH, 1]\n",
        "\n",
        "        # Calculate attention weights by applying softmax to the attention scores.\n",
        "        # Softmax ensures that the weights sum up to 1 across all encoder outputs for each time step.\n",
        "        # BLANK: Along which axis should we apply softmax?\n",
        "        attention_weights = tf.nn.softmax(attention_scores, axis=_____BLANK______)  # Apply softmax across the sequence length (axis=1) to get weights for each encoder output.\n",
        "\n",
        "        # Calculate the context vector by taking a weighted sum of the encoder outputs.\n",
        "        # This vector captures the most relevant information from the source sequence for the current decoding step.\n",
        "        context_vector = tf.reduce_sum(\n",
        "            attention_weights * encoder_outputs, # Element-wise multiplication of weights and encoder outputs.\n",
        "            axis=1  # Sum along the sequence length (axis=1) to get a single context vector for each sample in the batch.\n",
        "        ) # Shape: [batch_size, lstm_units]\n",
        "\n",
        "        # Squeeze the attention weights to remove the last dimension (which was 1).\n",
        "        attention_weights = tf.squeeze(attention_weights, axis=-1) # Shape: [batch_size, MAX_LENGTH]\n",
        "\n",
        "        return context_vector, attention_weights\n"
      ],
      "metadata": {
        "id": "3Uczy0uWX-da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B.3: DECODER WITH ATTENTION\n"
      ],
      "metadata": {
        "id": "juGd5QcnYET3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder_WithAttention_Vectorized(keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
        "        super(Decoder_WithAttention_Vectorized, self).__init__()\n",
        "\n",
        "        self.embedding = layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            mask_zero=True\n",
        "        )\n",
        "\n",
        "        # Vectorized attention components\n",
        "        self.attention_W1 = layers.Dense(lstm_units)\n",
        "        self.attention_W2 = layers.Dense(lstm_units)\n",
        "        self.attention_V = layers.Dense(1)\n",
        "\n",
        "        self.lstm = layers.LSTM(\n",
        "            lstm_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True\n",
        "        )\n",
        "\n",
        "        self.dense = layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x, encoder_outputs, initial_state):\n",
        "        # Embed input\n",
        "        x = self.embedding(x)  # [batch, seq_len, embedding_dim]\n",
        "\n",
        "        state_h, state_c = initial_state\n",
        "\n",
        "        # Calculate attention using encoder's final state\n",
        "        state_h_expanded = tf.expand_dims(state_h, 1)  # [batch, 1, lstm_units]\n",
        "\n",
        "        # Vectorized attention calculation\n",
        "        energy = tf.nn.tanh(\n",
        "            self.attention_W1(encoder_outputs) +\n",
        "            self.attention_W2(state_h_expanded)\n",
        "        )\n",
        "        attention_scores = self.attention_V(energy)\n",
        "        attention_weights = tf.nn.softmax(attention_scores, axis=1)\n",
        "        context_vector = tf.reduce_sum(\n",
        "            attention_weights * encoder_outputs,\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Tile context for all decoder timesteps\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        context_vector = tf.expand_dims(context_vector, 1)\n",
        "        context_vector = tf.tile(context_vector, [1, seq_len, 1])\n",
        "\n",
        "        # Concatenate and process (like no-attention decoder)\n",
        "        lstm_input = tf.concat([x, context_vector], axis=-1)\n",
        "        lstm_output, _, _ = self.lstm(lstm_input, initial_state=[state_h, state_c])\n",
        "        outputs = self.dense(lstm_output)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class Decoder_WithAttention(keras.Model):\n",
        "    \"\"\"Decoder WITH Bahdanau Attention. This decoder uses the attention mechanism to dynamically focus on different parts of the source sentence while generating the target sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
        "        super(Decoder_WithAttention, self).__init__()\n",
        "\n",
        "        # Embedding layer for the target language. Converts target token IDs to dense vectors.\n",
        "        self.embedding = layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            mask_zero=True # Mask padding tokens.\n",
        "        )\n",
        "\n",
        "        # Initialize the Bahdanau Attention layer. This layer will calculate context vectors.\n",
        "        self.attention = BahdanauAttention(lstm_units)\n",
        "\n",
        "        # LSTM layer for the decoder. It takes the embedded input token and\n",
        "        # the context vector (from attention) along with its previous hidden state.\n",
        "        # return_sequences=True is used in the call method during a loop, rather than in the LSTM definition directly\n",
        "        # because we are decoding one token at a time implicitly within the loop.\n",
        "        self.lstm = layers.LSTM(\n",
        "            lstm_units,\n",
        "            return_sequences=True,  # Return sequences as we'll be processing inputs over time steps within the call method loop.\n",
        "            return_state=True,      # Return the final hidden and cell states to be used in the next decoding step.\n",
        "            name='decoder_lstm_with_attention'\n",
        "        )\n",
        "\n",
        "        # Dense layer: Maps the LSTM's output to a probability distribution over the target vocabulary.\n",
        "        self.dense = layers.Dense(vocab_size, name='output_dense_with_attention')\n",
        "\n",
        "    def call(self, x, encoder_outputs, initial_state):\n",
        "        # x: Input batch of target sequences [batch_size, MAX_LENGTH] (for training, it's the shifted target sequence).\n",
        "        # encoder_outputs: All hidden states from the encoder [batch_size, MAX_LENGTH, lstm_units]. Used by attention.\n",
        "        # initial_state: A list containing [state_h, state_c] from the encoder's final states, used to initialize the decoder LSTM.\n",
        "\n",
        "        # 1. Embed the input sequence (current target tokens for decoding).\n",
        "        x = self.embedding(x) # Shape: [batch_size, MAX_LENGTH, embedding_dim]\n",
        "\n",
        "        # Initialize decoder's hidden and cell states with the encoder's final states.\n",
        "        state_h, state_c = initial_state\n",
        "\n",
        "        # List to store outputs (predictions) for each time step.\n",
        "        outputs = []\n",
        "\n",
        "        # Loop through the sequence length to decode one token at a time.\n",
        "        # During training, `x.shape[1]` is `MAX_LENGTH - 1` because target_batch[:, :-1] is used for decoder input.\n",
        "        for t in range(x.shape[1]):\n",
        "            # Take the input for the current time step (t).\n",
        "            input_t = x[:, t:t+1, :] # Shape: [batch_size, 1, embedding_dim]\n",
        "\n",
        "            # Calculate the context vector using the attention mechanism.\n",
        "            # The attention mechanism uses the decoder's current hidden state (state_h) and all encoder outputs.\n",
        "            context_vector, _ = self.attention(state_h, encoder_outputs) # context_vector shape: [batch_size, lstm_units]\n",
        "            context_vector = tf.expand_dims(context_vector, axis=1) # Expand context_vector to [batch_size, 1, lstm_units] to concatenate with input_t.\n",
        "\n",
        "            # Concatenate the embedded input token with the context vector.\n",
        "            # This combines the information about the previous token with the relevant parts of the source sentence.\n",
        "            lstm_input = tf.concat([input_t, context_vector], axis=-1) # Shape: [batch_size, 1, embedding_dim + lstm_units]\n",
        "\n",
        "            # Pass the combined input through the decoder LSTM.\n",
        "            # The initial_state for this LSTM call is the (state_h, state_c) from the previous time step.\n",
        "            output_t, state_h, state_c = self.lstm(\n",
        "                lstm_input,\n",
        "                initial_state=[state_h, state_c] # Pass the updated states from the previous step.\n",
        "            ) # output_t shape: [batch_size, 1, lstm_units], state_h/c shape: [batch_size, lstm_units]\n",
        "\n",
        "            outputs.append(output_t) # Collect the LSTM output for this time step.\n",
        "\n",
        "        # Concatenate all time step outputs to form the full sequence of LSTM outputs.\n",
        "        outputs = tf.concat(outputs, axis=1) # Shape: [batch_size, MAX_LENGTH, lstm_units]\n",
        "\n",
        "        # Apply the dense layer to get the final predictions (logits) for each token in the vocabulary.\n",
        "        outputs = self.dense(outputs) # Shape: [batch_size, MAX_LENGTH, vocab_size]\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "q7a-SPZPYHwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B.4: BUILD MODEL WITH ATTENTION\n"
      ],
      "metadata": {
        "id": "bciwBFy-Ycww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBuilding Seq2Seq model WITH attention...\")\n",
        "\n",
        "encoder_attn = Encoder_WithAttention(input_vocab_size, EMBEDDING_DIM, LSTM_UNITS)\n",
        "decoder_attn = Decoder_WithAttention_Vectorized(target_vocab_size, EMBEDDING_DIM, LSTM_UNITS)\n",
        "\n",
        "print(\"✓ Encoder, Attention, and Decoder (WITH attention) created successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qzz_hRrYgro",
        "outputId": "0329ae74-80a2-4666-e43d-b722a10674a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Building Seq2Seq model WITH attention...\n",
            "✓ Encoder, Attention, and Decoder (WITH attention) created successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B.5: TRAINING SETUP FOR MODEL WITH ATTENTION"
      ],
      "metadata": {
        "id": "cRpi9YwZYjM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_attn = keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0) # Adam optimizer is also used for the attention model, known for its efficiency.\n",
        "\n",
        "# Loss function: SparseCategoricalCrossentropy is suitable for integer-encoded targets.\n",
        "# from_logits=True means the model outputs raw logits (unnormalized scores), and the loss function will apply softmax internally.\n",
        "loss_fn_attn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def train_step_with_attention(input_batch, target_batch):\n",
        "    \"\"\"Single training step for model WITH attention, including forward pass, loss calculation, and backpropagation.\"\"\"\n",
        "    with tf.GradientTape() as tape: # Record operations for automatic differentiation.\n",
        "        # Encoder forward pass: Processes the input sequence to generate encoder outputs and final states.\n",
        "        # encoder_outputs are crucial here as they will be used by the attention mechanism in the decoder.\n",
        "        encoder_outputs, state_h, state_c = encoder_attn(input_batch)\n",
        "\n",
        "        # Prepare decoder input and target sequences.\n",
        "        # Decoder input is the target sequence shifted by one position to the right (e.g., starts with <start> token).\n",
        "        decoder_input = target_batch[:, :-1]\n",
        "        # Decoder target is the actual target sequence, shifted left, used for calculating loss.\n",
        "        decoder_target = target_batch[:, 1:]\n",
        "\n",
        "        # Decoder forward pass: Generates predictions using the decoder, which now incorporates attention.\n",
        "        # It takes the decoder input, all encoder outputs, and the initial LSTM states from the encoder.\n",
        "        predictions = decoder_attn(decoder_input, encoder_outputs, [state_h, state_c])\n",
        "\n",
        "        # Calculate loss, masking out padding tokens (0s) to ensure they don't contribute to the loss.\n",
        "        mask = tf.math.not_equal(decoder_target, 0)\n",
        "        loss = loss_fn_attn(decoder_target, predictions, sample_weight=mask)\n",
        "\n",
        "        # Calculate token-level accuracy, also ignoring padding tokens.\n",
        "        accuracy = calculate_accuracy(predictions, decoder_target)\n",
        "\n",
        "    # Collect all trainable variables from both the encoder and decoder.\n",
        "    trainable_vars = encoder_attn.trainable_variables + decoder_attn.trainable_variables\n",
        "    # Compute gradients of the loss with respect to these trainable variables.\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "    # Apply the calculated gradients to update the model's weights.\n",
        "    optimizer_attn.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "    return loss, accuracy\n",
        "\n",
        "def evaluate_with_attention(input_data, target_data):\n",
        "    \"\"\"Evaluates the model WITH attention on a given dataset (e.g., validation set), returning average loss and accuracy.\"\"\"\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    num_batches = len(input_data) // BATCH_SIZE\n",
        "\n",
        "    # Iterate through the evaluation dataset in batches.\n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * BATCH_SIZE\n",
        "        end_idx = start_idx + BATCH_SIZE\n",
        "\n",
        "        input_batch = input_data[start_idx:end_idx]\n",
        "        target_batch = target_data[start_idx:end_idx]\n",
        "\n",
        "        # Encoder forward pass (no gradient computation needed during evaluation).\n",
        "        encoder_outputs, state_h, state_c = encoder_attn(input_batch)\n",
        "        # Prepare decoder input and target, similar to training.\n",
        "        decoder_input = target_batch[:, :-1]\n",
        "        decoder_target = target_batch[:, 1:]\n",
        "        # Decoder forward pass to get predictions.\n",
        "        predictions = decoder_attn(decoder_input, encoder_outputs, [state_h, state_c])\n",
        "\n",
        "        # Calculate loss and accuracy, ignoring padding tokens.\n",
        "        mask = tf.math.not_equal(decoder_target, 0)\n",
        "        loss = loss_fn_attn(decoder_target, predictions, sample_weight=mask)\n",
        "        accuracy = calculate_accuracy(predictions, decoder_target)\n",
        "\n",
        "        total_loss += loss\n",
        "        total_accuracy += accuracy\n",
        "\n",
        "    # Return the average loss and accuracy over all evaluation batches.\n",
        "    return total_loss / num_batches, total_accuracy / num_batches\n"
      ],
      "metadata": {
        "id": "XISlDZGcYmov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B.6: TRAINING MODEL WITH ATTENTION\n"
      ],
      "metadata": {
        "id": "As0a8z2ZYrUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING SEQ2SEQ WITH ATTENTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "best_val_loss_attn = float('inf') # Initialize best validation loss to infinity to ensure first epoch's loss is always better.\n",
        "best_val_acc_attn = 0            # Initialize best validation accuracy to 0.\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    # Shuffle training data at the beginning of each epoch to ensure variety in batches.\n",
        "    indices = np.random.permutation(len(train_input))\n",
        "    train_input_shuffled = train_input[indices]\n",
        "    train_target_shuffled = train_target[indices]\n",
        "\n",
        "    num_batches = len(train_input) // BATCH_SIZE\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "\n",
        "    # Loop through batches for training.\n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * BATCH_SIZE\n",
        "        end_idx = start_idx + BATCH_SIZE\n",
        "\n",
        "        input_batch = train_input_shuffled[start_idx:end_idx]\n",
        "        target_batch = train_target_shuffled[start_idx:end_idx]\n",
        "\n",
        "        # Perform a single training step for the attention model.\n",
        "        loss, accuracy = train_step_with_attention(input_batch, target_batch)\n",
        "        total_loss += loss\n",
        "        total_accuracy += accuracy\n",
        "\n",
        "    # Calculate average training loss and accuracy for the epoch.\n",
        "    train_loss = total_loss / num_batches\n",
        "    train_accuracy = total_accuracy / num_batches\n",
        "\n",
        "    # Evaluate the model on the validation set after each epoch.\n",
        "    val_loss, val_accuracy = evaluate_with_attention(val_input, val_target)\n",
        "\n",
        "    # Print epoch-wise training and validation metrics.\n",
        "    print(f'Epoch {epoch+1}/{EPOCHS} - Loss: {train_loss:.4f}, Acc: {train_accuracy:.4f} | '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
        "\n",
        "    # Update best validation loss and accuracy if current epoch's validation loss is lower.\n",
        "    if val_loss < best_val_loss_attn:\n",
        "        best_val_loss_attn = val_loss\n",
        "        best_val_acc_attn = val_accuracy\n",
        "\n",
        "print(f\"\\n✓ Training complete!\")\n",
        "print(f\"  Best Validation Loss (WITH ATTENTION): {best_val_loss_attn:.4f}\")\n",
        "print(f\"  Best Validation Accuracy (WITH ATTENTION): {best_val_acc_attn:.4f} ({best_val_acc_attn*100:.2f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "2c8241b146a7429b84a61dba65c62bf4",
            "3d9e2d280ff94571a8e2a6cd03cb3e09",
            "b839cba12eae42b389e9aea72db5ceb7",
            "c07b07e664f041a6af4ee887c4862d70",
            "6e65e5127c5548c19267e232a704bbb4",
            "100893da40d34717b56ad5707f0ce0a0",
            "e6a8015728344ece9e9a60aa06d75d59",
            "bd2e9bc38b834b9da0ca93ecf073d2c6",
            "894a73577eba46d7990b0c301a12fbf2",
            "293c9b2c048a4061956f87ef4539877a",
            "c562c0837fc548d789dff7babc665633"
          ]
        },
        "id": "1OKEGs9wYtCG",
        "outputId": "888e9cab-9f13-4d41-f3e1-da0c3a05e74c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "TRAINING SEQ2SEQ WITH ATTENTION\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c8241b146a7429b84a61dba65c62bf4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Loss: 1.1896, Acc: 0.2845 | Val Loss: 1.2773, Val Acc: 0.2712\n",
            "Epoch 2/20 - Loss: 0.9222, Acc: 0.4068 | Val Loss: 1.0786, Val Acc: 0.3589\n",
            "Epoch 3/20 - Loss: 0.7552, Acc: 0.4905 | Val Loss: 0.9604, Val Acc: 0.4328\n",
            "Epoch 4/20 - Loss: 0.6464, Acc: 0.5449 | Val Loss: 0.8931, Val Acc: 0.4736\n",
            "Epoch 5/20 - Loss: 0.5625, Acc: 0.5844 | Val Loss: 0.8445, Val Acc: 0.4961\n",
            "Epoch 6/20 - Loss: 0.4919, Acc: 0.6189 | Val Loss: 0.8094, Val Acc: 0.5177\n",
            "Epoch 7/20 - Loss: 0.4302, Acc: 0.6527 | Val Loss: 0.7906, Val Acc: 0.5287\n",
            "Epoch 8/20 - Loss: 0.3748, Acc: 0.6879 | Val Loss: 0.7701, Val Acc: 0.5421\n",
            "Epoch 9/20 - Loss: 0.3253, Acc: 0.7196 | Val Loss: 0.7510, Val Acc: 0.5511\n",
            "Epoch 10/20 - Loss: 0.2819, Acc: 0.7490 | Val Loss: 0.7396, Val Acc: 0.5564\n",
            "Epoch 11/20 - Loss: 0.2436, Acc: 0.7762 | Val Loss: 0.7304, Val Acc: 0.5629\n",
            "Epoch 12/20 - Loss: 0.2109, Acc: 0.8003 | Val Loss: 0.7300, Val Acc: 0.5716\n",
            "Epoch 13/20 - Loss: 0.1830, Acc: 0.8234 | Val Loss: 0.7209, Val Acc: 0.5768\n",
            "Epoch 14/20 - Loss: 0.1595, Acc: 0.8420 | Val Loss: 0.7191, Val Acc: 0.5774\n",
            "Epoch 15/20 - Loss: 0.1399, Acc: 0.8568 | Val Loss: 0.7262, Val Acc: 0.5794\n",
            "Epoch 16/20 - Loss: 0.1236, Acc: 0.8685 | Val Loss: 0.7226, Val Acc: 0.5852\n",
            "Epoch 17/20 - Loss: 0.1102, Acc: 0.8782 | Val Loss: 0.7284, Val Acc: 0.5843\n",
            "Epoch 18/20 - Loss: 0.0991, Acc: 0.8861 | Val Loss: 0.7286, Val Acc: 0.5809\n",
            "Epoch 19/20 - Loss: 0.0903, Acc: 0.8920 | Val Loss: 0.7336, Val Acc: 0.5871\n",
            "Epoch 20/20 - Loss: 0.0826, Acc: 0.8969 | Val Loss: 0.7405, Val Acc: 0.5786\n",
            "\n",
            "✓ Training complete!\n",
            "  Best Validation Loss (WITH ATTENTION): 0.7191\n",
            "  Best Validation Accuracy (WITH ATTENTION): 0.5774 (57.74%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART C: COMPARISON AND TESTING\n"
      ],
      "metadata": {
        "id": "spRzM6ynYwQr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvs5IGatUYAB",
        "outputId": "1903fc04-495f-4ee0-e4c1-1f2c97fdd424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics Comparison:\n",
            "Metric                         Without Attention    With Attention       Improvement    \n",
            "-------------------------------------------------------------------------------------\n",
            "Validation Loss                2.5106               0.7191                        71.36%\n",
            "Validation Accuracy            0.5585               0.5774                         3.40%\n",
            "\n",
            "======================================================================\n",
            "TRANSLATION EXAMPLES WITH REFERENCE TRANSLATIONS\n",
            "======================================================================\n",
            "\n",
            "Note: BLEU score ranges from 0 (worst) to 1 (perfect match)\n",
            "Higher BLEU score = better translation quality\n",
            "\n",
            "Example 2:\n",
            "  English:             <start> who is that guy ? <end>\n",
            "  Reference German:    <start> wer ist dieser typ ? <end>\n",
            "  Without Attention:   wer ist das hier (BLEU: 0.3894)\n",
            "  With Attention:      wer ist dieser kerl (BLEU: 0.5841)\n",
            "\n",
            "Example 12:\n",
            "  English:             <start> who stabbed tom ? <end>\n",
            "  Reference German:    <start> wer hat tom mit dem messer gestochen ? <end>\n",
            "  Without Attention:   wer hat tom geküsst (BLEU: 0.2759)\n",
            "  With Attention:      wer hat maria (BLEU: 0.1259)\n",
            "\n",
            "Example 22:\n",
            "  English:             <start> who was at home ? <end>\n",
            "  Reference German:    <start> wer war zu hause ? <end>\n",
            "  Without Attention:   ich war zu hause (BLEU: 0.5841)\n",
            "  With Attention:      wer war zuhause (BLEU: 0.3423)\n",
            "\n",
            "Example 32:\n",
            "  English:             <start> who's that girl ? <end>\n",
            "  Reference German:    <start> wer ist dieses mädchen ? <end>\n",
            "  Without Attention:   wer ist das liebe (BLEU: 0.3894)\n",
            "  With Attention:      wer ist das hier wer (BLEU: 0.4000)\n",
            "\n",
            "Example 42:\n",
            "  English:             <start> why are you mad ? <end>\n",
            "  Reference German:    <start> warum bist du wütend ? <end>\n",
            "  Without Attention:   was ist tom beim typ (BLEU: 0.0000)\n",
            "  With Attention:      warum ist sie wütend (BLEU: 0.3894)\n",
            "\n",
            "======================================================================\n",
            "FINAL COMPARISON SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Metric                         Without Attention    With Attention      \n",
            "----------------------------------------------------------------------\n",
            "Validation Loss                2.5106               0.7191              \n",
            "Validation Accuracy            55.85               % 57.74               %\n",
            "Average BLEU Score             0.3278               0.3683              \n",
            "\n",
            "======================================================================\n",
            "TRAINING COMPLETE!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nMetrics Comparison:\")\n",
        "print(f\"{'Metric':<30} {'Without Attention':<20} {'With Attention':<20} {'Improvement':<15}\")\n",
        "print(\"-\" * 85)\n",
        "# Display validation loss for both models and calculate percentage improvement.\n",
        "print(f\"{'Validation Loss':<30} {best_val_loss_no_attn:<20.4f} {best_val_loss_attn:<20.4f} \"\n",
        "      f\"{((best_val_loss_no_attn - best_val_loss_attn) / best_val_loss_no_attn * 100):>14.2f}%\")\n",
        "# Display validation accuracy for both models and calculate percentage improvement.\n",
        "print(f\"{'Validation Accuracy':<30} {best_val_acc_no_attn:<20.4f} {best_val_acc_attn:<20.4f} \"\n",
        "      f\"{((best_val_acc_attn - best_val_acc_no_attn) / best_val_acc_no_attn * 100):>14.2f}%\")\n",
        "\n",
        "# ==============================================================================\n",
        "# TRANSLATION FUNCTIONS: Helper functions to translate a given English sentence into German using the trained models.\n",
        "# ==============================================================================\n",
        "\n",
        "def translate_no_attention(sentence):\n",
        "    \"\"\"Translate using model WITHOUT attention. This function simulates inference by decoding token by token.\"\"\"\n",
        "    # Preprocess the input sentence (lowercase, add <start>/<end> tokens, handle punctuation).\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    # Convert the processed sentence to a sequence of token IDs.\n",
        "    inputs = input_tokenizer.texts_to_sequences([sentence])\n",
        "    # Pad the input sequence to MAX_LENGTH.\n",
        "    inputs = keras.preprocessing.sequence.pad_sequences(inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "    # Get encoder's final hidden and cell states, which serve as the context for the decoder.\n",
        "    encoder_outputs, state_h, state_c = encoder_no_attn(inputs)\n",
        "\n",
        "    # Initialize the decoder's input with the <start> token.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = target_tokenizer.word_index['<start>']\n",
        "\n",
        "    decoded_sentence = [] # List to store the translated words.\n",
        "\n",
        "    # Decode word by word up to MAX_LENGTH.\n",
        "    for _ in range(MAX_LENGTH):\n",
        "        # Get predictions from the decoder using the current target sequence and encoder's final states.\n",
        "        predictions = decoder_no_attn(target_seq, [state_h, state_c])\n",
        "        # Get the ID of the word with the highest probability (greedy decoding).\n",
        "        predicted_id = tf.argmax(predictions[0, -1, :]).numpy()\n",
        "        # Convert the predicted ID back to a word.\n",
        "        predicted_word = target_tokenizer.index_word.get(predicted_id, '')\n",
        "\n",
        "        # Stop decoding if the <end> token is predicted.\n",
        "        if predicted_word == '<end>':\n",
        "            break\n",
        "\n",
        "        # Add the predicted word to the decoded sentence if it's not a special token.\n",
        "        if predicted_word and predicted_word != '<start>':\n",
        "            decoded_sentence.append(predicted_word)\n",
        "\n",
        "        # Prepare the input for the next decoding step: the last predicted word (or the entire sequence if using teacher forcing).\n",
        "        # For inference, the model predicts one word at a time, so `target_seq` updates to include the newly predicted word.\n",
        "        # This implementation uses an approach where `target_seq` grows with each predicted word, starting with `<start>`\n",
        "        # and then appending the predicted words. This is more common for training where the full shifted target sequence is passed.\n",
        "        # For pure inference, `target_seq` is usually just `[predicted_id]` for the next step.\n",
        "        # A simpler inference approach would be to update `target_seq` to only contain the `predicted_id` for the next step's input.\n",
        "        # The current implementation effectively reconstructs the input sequence for `decoder_no_attn` with each step, which can be computationally intensive.\n",
        "        target_seq = np.zeros((1, len(decoded_sentence) + 1))\n",
        "        target_seq[0, 0] = target_tokenizer.word_index['<start>']\n",
        "        for i, word in enumerate(decoded_sentence):\n",
        "            target_seq[0, i + 1] = target_tokenizer.word_index.get(word, 0)\n",
        "\n",
        "    return ' '.join(decoded_sentence)\n",
        "\n",
        "def translate_with_attention(sentence):\n",
        "    \"\"\"Translate using model WITH attention. This function also simulates inference token by token, incorporating attention.\"\"\"\n",
        "    # Preprocess the input sentence.\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    # Convert to token sequences and pad.\n",
        "    inputs = input_tokenizer.texts_to_sequences([sentence])\n",
        "    inputs = keras.preprocessing.sequence.pad_sequences(inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "    # Get encoder outputs and initial decoder states. encoder_outputs will be used by the attention mechanism.\n",
        "    encoder_outputs, state_h, state_c = encoder_attn(inputs)\n",
        "\n",
        "    # Initialize decoder's input with the <start> token.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = target_tokenizer.word_index['<start>']\n",
        "\n",
        "    decoded_sentence = [] # List to store the translated words.\n",
        "\n",
        "    # Decode word by word up to MAX_LENGTH.\n",
        "    for _ in range(MAX_LENGTH):\n",
        "        # Get predictions from the attention-based decoder.\n",
        "        # It takes the current target sequence (which typically is just the last predicted word during inference),\n",
        "        # all encoder outputs, and the current hidden/cell states.\n",
        "        predictions = decoder_attn(target_seq, encoder_outputs, [state_h, state_c])\n",
        "        # Get the ID of the most probable word.\n",
        "        predicted_id = tf.argmax(predictions[0, -1, :]).numpy()\n",
        "        # Convert ID back to word.\n",
        "        predicted_word = target_tokenizer.index_word.get(predicted_id, '')\n",
        "\n",
        "        # Stop if <end> token is predicted.\n",
        "        if predicted_word == '<end>':\n",
        "            break\n",
        "\n",
        "        # Add valid predicted words.\n",
        "        if predicted_word and predicted_word != '<start>':\n",
        "            decoded_sentence.append(predicted_word)\n",
        "\n",
        "        # Similar to the `translate_no_attention` function, this updates `target_seq` to include\n",
        "        # all words generated so far, starting with `<start>`. For true step-by-step inference,\n",
        "        # `target_seq` would ideally only contain the `predicted_id` from the current step to feed into the next.\n",
        "        target_seq = np.zeros((1, len(decoded_sentence) + 1))\n",
        "        target_seq[0, 0] = target_tokenizer.word_index['<start>']\n",
        "        for i, word in enumerate(decoded_sentence):\n",
        "            target_seq[0, i + 1] = target_tokenizer.word_index.get(word, 0)\n",
        "\n",
        "    return ' '.join(decoded_sentence)\n",
        "\n",
        "# ==============================================================================\n",
        "# TRANSLATION EXAMPLES WITH BLEU SCORES\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRANSLATION EXAMPLES WITH REFERENCE TRANSLATIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Select a few indices from the validation set to test translation.\n",
        "test_indices = [1, 11, 21, 31, 41]\n",
        "\n",
        "print(\"\\nNote: BLEU score ranges from 0 (worst) to 1 (perfect match)\")\n",
        "print(\"Higher BLEU score = better translation quality\\n\")\n",
        "\n",
        "total_bleu_no_attn = 0 # Accumulator for BLEU scores of the no-attention model.\n",
        "total_bleu_attn = 0    # Accumulator for BLEU scores of the attention model.\n",
        "\n",
        "# Iterate through the selected test sentences.\n",
        "for idx in test_indices:\n",
        "    if idx < len(val_input_texts):\n",
        "        # Retrieve the original English sentence and its reference German translation.\n",
        "        english_sentence = val_input_texts[idx]\n",
        "        reference_german = val_target_texts[idx]\n",
        "\n",
        "        # Get translations from both models.\n",
        "        translation_no_attn = translate_no_attention(english_sentence)\n",
        "        translation_attn = translate_with_attention(english_sentence)\n",
        "\n",
        "        # Calculate BLEU score for each translation against the reference.\n",
        "        bleu_no_attn = calculate_bleu_score(reference_german, translation_no_attn)\n",
        "        bleu_attn = calculate_bleu_score(reference_german, translation_attn)\n",
        "\n",
        "        total_bleu_no_attn += bleu_no_attn\n",
        "        total_bleu_attn += bleu_attn\n",
        "\n",
        "        # Print the results for each example.\n",
        "        print(f\"Example {idx+1}:\")\n",
        "        print(f\"  English:             {english_sentence}\")\n",
        "        print(f\"  Reference German:    {reference_german}\")\n",
        "        print(f\"  Without Attention:   {translation_no_attn} (BLEU: {bleu_no_attn:.4f})\")\n",
        "        print(f\"  With Attention:      {translation_attn} (BLEU: {bleu_attn:.4f})\")\n",
        "        print()\n",
        "\n",
        "# Calculate and print the average BLEU scores across all test examples.\n",
        "avg_bleu_no_attn = total_bleu_no_attn / len(test_indices)\n",
        "avg_bleu_attn = total_bleu_attn / len(test_indices)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL COMPARISON SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n{'Metric':<30} {'Without Attention':<20} {'With Attention':<20}\")\n",
        "print(\"-\" * 70)\n",
        "# Final summary of validation loss, accuracy, and average BLEU scores.\n",
        "print(f\"{'Validation Loss':<30} {best_val_loss_no_attn:<20.4f} {best_val_loss_attn:<20.4f}\")\n",
        "print(f\"{'Validation Accuracy':<30} {best_val_acc_no_attn*100:<20.2f}% {best_val_acc_attn*100:<20.2f}%\")\n",
        "print(f\"{'Average BLEU Score':<30} {avg_bleu_no_attn:<20.4f} {avg_bleu_attn:<20.4f}\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)\n"
      ]
    }
  ]
}