{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+wniccMxNwroFMAb0qUW4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/profliuhao/CSIT599/blob/main/CSIT599_Module6_BERT_NER_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BERT for Named Entity Recognition (NER) - Student Exercise\n",
        "\n",
        "===========================================================\n",
        "\n",
        "In this exercise, you will fine-tune a BERT model for Named Entity Recognition\n",
        "using the CoNLL-2003 dataset downloaded directly from a URL.\n",
        "\n",
        "\n",
        "Dataset: CoNLL-2003 NER dataset (downloaded from URL)\n",
        "\n",
        "Task: Token classification for entity recognition (PER, ORG, LOC, MISC)\n"
      ],
      "metadata": {
        "id": "A51Pb9UuEiYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "WQ88Vt_dQBVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "from io import BytesIO\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    BertForTokenClassification,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from seqeval.metrics import classification_report, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "0yeDGfVZQO7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 1: Download and Parse CoNLL-2003 Dataset"
      ],
      "metadata": {
        "id": "IqGXw-AzWE6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PART 1: Download and Parse CoNLL-2003 Dataset\n",
        "# ==============================================================================\n",
        "\n",
        "def download_and_extract_data(url, extract_path=\"./conll2003_data\"):\n",
        "    \"\"\"\n",
        "    Download and extract the CoNLL-2003 dataset from a URL.\n",
        "\n",
        "    Args:\n",
        "        url: URL to download the dataset zip file\n",
        "        extract_path: Path to extract the dataset\n",
        "\n",
        "    Returns:\n",
        "        extract_path: Path where data was extracted\n",
        "    \"\"\"\n",
        "    print(f\"Downloading CoNLL-2003 dataset from {url}...\")\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "    # Download the zip file\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(BytesIO(response.content)) as zip_file:\n",
        "        zip_file.extractall(extract_path)\n",
        "\n",
        "    print(f\"Dataset extracted to {extract_path}\")\n",
        "    return extract_path\n",
        "\n",
        "\n",
        "def parse_conll_file(file_path):\n",
        "    \"\"\"\n",
        "    Parse a CoNLL-2003 format file.\n",
        "\n",
        "    The file format has one token per line with columns:\n",
        "    token pos_tag chunk_tag ner_tag\n",
        "\n",
        "    Sentences are separated by empty lines.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the CoNLL file\n",
        "\n",
        "    Returns:\n",
        "        sentences: List of sentences, where each sentence is a list of tokens\n",
        "        ner_tags: List of NER tag sequences, parallel to sentences\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    ner_tags = []\n",
        "\n",
        "    current_tokens = []\n",
        "    current_tags = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "\n",
        "            # Empty line indicates end of sentence\n",
        "            if not line or line.startswith('-DOCSTART-'):\n",
        "                if current_tokens:\n",
        "                    sentences.append(current_tokens)\n",
        "                    ner_tags.append(current_tags)\n",
        "                    current_tokens = []\n",
        "                    current_tags = []\n",
        "            else:\n",
        "                # Parse the line: token pos chunk ner\n",
        "                parts = line.split()\n",
        "                if len(parts) >= 4:\n",
        "                    token = parts[0]\n",
        "                    ner_tag = parts[3]\n",
        "\n",
        "                    current_tokens.append(token)\n",
        "                    current_tags.append(ner_tag)\n",
        "\n",
        "    # Add the last sentence if file doesn't end with empty line\n",
        "    if current_tokens:\n",
        "        sentences.append(current_tokens)\n",
        "        ner_tags.append(current_tags)\n",
        "\n",
        "    return sentences, ner_tags\n",
        "\n",
        "\n",
        "def load_conll_dataset(data_path=\"./conll2003_data\"):\n",
        "    \"\"\"\n",
        "    Load all splits of the CoNLL-2003 dataset.\n",
        "\n",
        "    Args:\n",
        "        data_path: Path to the extracted dataset\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with 'train', 'validation', and 'test' splits\n",
        "    \"\"\"\n",
        "    print(\"Loading CoNLL-2003 dataset files...\")\n",
        "\n",
        "    # The CoNLL-2003 dataset typically has these file names\n",
        "    train_file = os.path.join(data_path, \"train.txt\")\n",
        "    dev_file = os.path.join(data_path, \"valid.txt\")\n",
        "    test_file = os.path.join(data_path, \"test.txt\")\n",
        "\n",
        "    # Parse each file\n",
        "    train_sentences, train_tags = parse_conll_file(train_file)\n",
        "    dev_sentences, dev_tags = parse_conll_file(dev_file)\n",
        "    test_sentences, test_tags = parse_conll_file(test_file)\n",
        "\n",
        "    print(f\"Loaded {len(train_sentences)} training sentences\")\n",
        "    print(f\"Loaded {len(dev_sentences)} validation sentences\")\n",
        "    print(f\"Loaded {len(test_sentences)} test sentences\")\n",
        "\n",
        "    return {\n",
        "        'train': {'tokens': train_sentences, 'ner_tags': train_tags},\n",
        "        'validation': {'tokens': dev_sentences, 'ner_tags': dev_tags},\n",
        "        'test': {'tokens': test_sentences, 'ner_tags': test_tags}\n",
        "    }\n",
        "\n",
        "\n",
        "# Define label mappings\n",
        "label_list = [\n",
        "    \"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\",\n",
        "    \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"\n",
        "]\n",
        "\n",
        "# TODO: Create a dictionary mapping label names to IDs\n",
        "# Hint: Use enumerate to create {label: id} mapping\n",
        "label2id = _____________________\n",
        "\n",
        "# TODO: Create a dictionary mapping label IDs to names\n",
        "# Hint: Reverse the label2id mapping\n",
        "id2label = _____________________\n",
        "\n",
        "num_labels = len(label_list)"
      ],
      "metadata": {
        "id": "mA5ynmC8PoBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 2: Custom PyTorch Dataset Class"
      ],
      "metadata": {
        "id": "cZ1aSRAHWHD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PART 2: Custom PyTorch Dataset Class\n",
        "# ==============================================================================\n",
        "\n",
        "class CoNLLDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for CoNLL-2003 data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sentences, ner_tags, tokenizer, max_length=128, label2id=None):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            sentences: List of token lists\n",
        "            ner_tags: List of NER tag lists (as strings)\n",
        "            tokenizer: BERT tokenizer\n",
        "            max_length: Maximum sequence length\n",
        "            label2id: Dictionary mapping label strings to IDs\n",
        "        \"\"\"\n",
        "        self.sentences = sentences\n",
        "        self.ner_tags = ner_tags\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.label2id = label2id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a single example.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with input_ids, attention_mask, and labels\n",
        "        \"\"\"\n",
        "        tokens = self.sentences[idx]\n",
        "        tags = self.ner_tags[idx]\n",
        "\n",
        "        # Convert string tags to IDs\n",
        "        tag_ids = [self.label2id[tag] for tag in tags]\n",
        "\n",
        "        # Tokenize and align labels\n",
        "        encoded = self.tokenizer(\n",
        "            tokens,\n",
        "            is_split_into_words=True,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Align labels with tokenized input\n",
        "        labels = self.align_labels(encoded, tag_ids)\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'labels': torch.tensor(labels, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def align_labels(self, encoded, tag_ids, label_all_tokens=True):\n",
        "        \"\"\"\n",
        "        Align labels with tokenized input.\n",
        "\n",
        "        Args:\n",
        "            encoded: Output from tokenizer\n",
        "            tag_ids: List of label IDs for original tokens\n",
        "            label_all_tokens: Whether to label all subword tokens\n",
        "\n",
        "        Returns:\n",
        "            List of aligned label IDs\n",
        "        \"\"\"\n",
        "        word_ids = encoded.word_ids(batch_index=0)\n",
        "\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            # TODO: Handle special tokens (None) - assign -100 to ignore in loss\n",
        "            # Hint: Special tokens have word_idx = None\n",
        "            if word_idx is None:\n",
        "                label_ids.append(_____)\n",
        "\n",
        "            # TODO: For the first token of a word, use the actual label\n",
        "            # Hint: Check if word_idx != previous_word_idx\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(_____)\n",
        "\n",
        "            # TODO: For subsequent tokens of the same word\n",
        "            # If label_all_tokens is True, use the label; otherwise use -100\n",
        "            else:\n",
        "                label_ids.append(tag_ids[word_idx] if _____ else -100)\n",
        "\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        return label_ids\n",
        "\n"
      ],
      "metadata": {
        "id": "JQofVXz-PpGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 3: Prepare Data Loaders"
      ],
      "metadata": {
        "id": "VYsgcJ-KWKrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PART 3: Prepare Data Loaders\n",
        "# ==============================================================================\n",
        "\n",
        "def prepare_dataloaders(dataset_dict, tokenizer, batch_size=16):\n",
        "    \"\"\"\n",
        "    Prepare PyTorch DataLoaders for training and evaluation.\n",
        "\n",
        "    Args:\n",
        "        dataset_dict: Dictionary with train/validation/test splits\n",
        "        tokenizer: BERT tokenizer\n",
        "        batch_size: Batch size for training\n",
        "\n",
        "    Returns:\n",
        "        train_dataloader, eval_dataloader, test_dataloader\n",
        "    \"\"\"\n",
        "    print(\"Preparing data loaders...\")\n",
        "\n",
        "    # Create dataset objects\n",
        "    train_dataset = CoNLLDataset(\n",
        "        dataset_dict['train']['tokens'],\n",
        "        dataset_dict['train']['ner_tags'],\n",
        "        tokenizer,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    eval_dataset = CoNLLDataset(\n",
        "        dataset_dict['validation']['tokens'],\n",
        "        dataset_dict['validation']['ner_tags'],\n",
        "        tokenizer,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    test_dataset = CoNLLDataset(\n",
        "        dataset_dict['test']['tokens'],\n",
        "        dataset_dict['test']['ner_tags'],\n",
        "        tokenizer,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    # TODO: Create DataLoader for training set\n",
        "    # Hint: Use DataLoader with shuffle=True for training\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        shuffle=_____,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # TODO: Create DataLoader for validation set\n",
        "    # Hint: Use shuffle=False for evaluation\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset,\n",
        "        shuffle=_____,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Create DataLoader for test set\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    return train_dataloader, eval_dataloader, test_dataloader\n",
        "\n"
      ],
      "metadata": {
        "id": "0u-UD3QAPtTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 4: Training Function"
      ],
      "metadata": {
        "id": "K3VIpYi0WMck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PART 4: Training Function\n",
        "# ==============================================================================\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch.\n",
        "\n",
        "    Args:\n",
        "        model: BERT model for token classification\n",
        "        dataloader: Training data loader\n",
        "        optimizer: Optimizer\n",
        "        scheduler: Learning rate scheduler\n",
        "        device: CPU or CUDA device\n",
        "\n",
        "    Returns:\n",
        "        Average training loss for the epoch\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        # TODO: Move batch tensors to device (GPU/CPU)\n",
        "        # Hint: Use .to(device) for each tensor\n",
        "        input_ids = batch[\"input_ids\"].to(_____)\n",
        "        attention_mask = batch[\"attention_mask\"].to(_____)\n",
        "        labels = batch[\"labels\"].to(_____)\n",
        "\n",
        "        # TODO: Zero the gradients\n",
        "        # Hint: Use optimizer.zero_grad()\n",
        "        _____\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # TODO: Backward pass - compute gradients\n",
        "        # Hint: Call .backward() on the loss\n",
        "        _____\n",
        "\n",
        "        # TODO: Update model parameters\n",
        "        # Hint: Use optimizer.step()\n",
        "        _____\n",
        "\n",
        "        # TODO: Update learning rate\n",
        "        # Hint: Use scheduler.step()\n",
        "        _____\n",
        "\n",
        "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n"
      ],
      "metadata": {
        "id": "YX03uxfyPvtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 5: Evaluation Function"
      ],
      "metadata": {
        "id": "7ikFGZzKWPca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PART 5: Evaluation Function\n",
        "# ==============================================================================\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the given dataloader.\n",
        "\n",
        "    Uses seqeval library which properly handles entity-level evaluation\n",
        "    (rather than token-level), following the CoNLL evaluation scheme.\n",
        "\n",
        "    Args:\n",
        "        model: BERT model for token classification\n",
        "        dataloader: Evaluation data loader\n",
        "        device: CPU or CUDA device\n",
        "\n",
        "    Returns:\n",
        "        metrics: Dictionary with precision, recall, f1, and loss\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # TODO: Get model outputs (forward pass)\n",
        "            # Hint: Pass input_ids, attention_mask, and labels to model\n",
        "            outputs = model(\n",
        "                input_ids=_____,\n",
        "                attention_mask=_____,\n",
        "                labels=_____\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Get predictions (logits -> class IDs)\n",
        "            # TODO: Get the predicted class for each token\n",
        "            # Hint: Use torch.argmax on outputs.logits along the last dimension\n",
        "            predictions = torch.argmax(_____, dim=-1)\n",
        "\n",
        "            # Convert to lists and remove ignored index (-100)\n",
        "            predictions = predictions.cpu().numpy()\n",
        "            labels = labels.cpu().numpy()\n",
        "\n",
        "            # Remove padding and special tokens for evaluation\n",
        "            for pred_seq, label_seq in zip(predictions, labels):\n",
        "                pred_labels = []\n",
        "                true_labels = []\n",
        "\n",
        "                for pred, label in zip(pred_seq, label_seq):\n",
        "                    if label != -100:  # Ignore special tokens\n",
        "                        pred_labels.append(id2label[pred])\n",
        "                        true_labels.append(id2label[label])\n",
        "\n",
        "                all_predictions.append(pred_labels)\n",
        "                all_labels.append(true_labels)\n",
        "\n",
        "    # Calculate metrics using seqeval\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    f1 = f1_score(all_labels, all_predictions)\n",
        "\n",
        "    print(\"\\n\" + classification_report(all_labels, all_predictions))\n",
        "\n",
        "    return {\n",
        "        \"loss\": avg_loss,\n",
        "        \"f1\": f1\n",
        "    }"
      ],
      "metadata": {
        "id": "ApJZFK8XPwgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 6: Main Training Loop"
      ],
      "metadata": {
        "id": "Orc13Uh6WQ3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PART 6: Main Training Loop\n",
        "# ==============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to orchestrate the training process.\n",
        "    \"\"\"\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Download and load dataset\n",
        "    data_url = \"https://data.deepai.org/conll2003.zip\"\n",
        "    data_path = download_and_extract_data(data_url)\n",
        "    dataset = load_conll_dataset(data_path)\n",
        "\n",
        "    # TODO: Initialize BERT tokenizer\n",
        "    # Hint: Use BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
        "    # Note: Use cased version because entity names are case-sensitive\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(_____)\n",
        "\n",
        "    # Prepare data loaders\n",
        "    train_dataloader, eval_dataloader, test_dataloader = prepare_dataloaders(\n",
        "        dataset, tokenizer, batch_size=16\n",
        "    )\n",
        "\n",
        "    # TODO: Initialize BERT model for token classification\n",
        "    # Hint: Use BertForTokenClassification.from_pretrained with num_labels parameter\n",
        "    model = BertForTokenClassification.from_pretrained(\n",
        "        \"bert-base-cased\",\n",
        "        num_labels=_____,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    # Training hyperparameters\n",
        "    num_epochs = 3\n",
        "    learning_rate = 5e-5\n",
        "\n",
        "    # TODO: Initialize optimizer\n",
        "    # Hint: Use AdamW with model.parameters() and learning_rate\n",
        "    optimizer = AdamW(_____, lr=_____)\n",
        "\n",
        "    # Calculate total training steps for scheduler\n",
        "    total_steps = len(train_dataloader) * num_epochs\n",
        "\n",
        "    # TODO: Initialize learning rate scheduler\n",
        "    # Hint: Use get_linear_schedule_with_warmup\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=_____,  # Use 10% of total steps for warmup\n",
        "        num_training_steps=_____\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Starting Training\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    best_f1 = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Train\n",
        "        train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
        "        print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "        # Evaluate\n",
        "        print(\"\\nValidation Results:\")\n",
        "        eval_metrics = evaluate(model, eval_dataloader, device)\n",
        "        print(f\"Validation Loss: {eval_metrics['loss']:.4f}\")\n",
        "        print(f\"Validation F1: {eval_metrics['f1']:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if eval_metrics['f1'] > best_f1:\n",
        "            best_f1 = eval_metrics['f1']\n",
        "            torch.save(model.state_dict(), \"best_bert_ner_model.pt\")\n",
        "            print(f\"Saved new best model with F1: {best_f1:.4f}\")\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Final Evaluation on Test Set\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(\"best_bert_ner_model.pt\"))\n",
        "    test_metrics = evaluate(model, test_dataloader, device)\n",
        "    print(f\"\\nTest Loss: {test_metrics['loss']:.4f}\")\n",
        "    print(f\"Test F1: {test_metrics['f1']:.4f}\")"
      ],
      "metadata": {
        "id": "U7UL6bGyPzas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 7: Inference Example"
      ],
      "metadata": {
        "id": "10SUpn-5WVSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PART 7: Inference Example\n",
        "# ==============================================================================\n",
        "\n",
        "def predict_entities(text, model, tokenizer, device):\n",
        "    \"\"\"\n",
        "    Predict named entities in a given text.\n",
        "\n",
        "    Args:\n",
        "        text: Input text string\n",
        "        model: Trained BERT NER model\n",
        "        tokenizer: BERT tokenizer\n",
        "        device: CPU or CUDA device\n",
        "\n",
        "    Returns:\n",
        "        List of (word, entity_tag) tuples\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize input\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    input_ids = tokens[\"input_ids\"].to(device)\n",
        "    attention_mask = tokens[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "    # Convert token IDs back to words\n",
        "    predicted_labels = predictions[0].cpu().numpy()\n",
        "    tokens_decoded = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "    # Filter out special tokens and pair with predictions\n",
        "    results = []\n",
        "    for token, label_id in zip(tokens_decoded, predicted_labels):\n",
        "        if token not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
        "            results.append((token, id2label[label_id]))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FMvO0Y_QP2z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 8: Training"
      ],
      "metadata": {
        "id": "4_QtsFMsWYy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "rqNLTNv1P5_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 9: Inference"
      ],
      "metadata": {
        "id": "BjthcRCjWc9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example inference\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "model.load_state_dict(torch.load(\"best_bert_ner_model.pt\"))\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "j1eza9vDPYDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"Apple Inc. is located in Cupertino, California. Tim Cook is the CEO.\"\n",
        "entities = predict_entities(test_text, model, tokenizer, device)\n",
        "print(\"\\nDetected Entities:\")\n",
        "for token, label in entities:\n",
        "    if label != \"O\":\n",
        "        print(f\"{token}: {label}\")"
      ],
      "metadata": {
        "id": "QEFrK4CZE3_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GvCS7GOZE4qj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}