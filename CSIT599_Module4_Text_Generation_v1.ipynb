{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDkjZIIxPCQSrtBTl2lZ2q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/profliuhao/CSIT599/blob/main/CSIT599_Module4_Text_Generation_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Character-Level LSTM Text Generation - Shakespeare Style\n",
        "\n",
        "## Exercise\n",
        "\n",
        "Instructions for Students:\n",
        "\n",
        "Fill in the blanks marked with \"# TODO: Student fills this\" to complete the code.\n",
        "\n",
        "You will need to specify:\n",
        "1. Sequence length for training\n",
        "2. LSTM architecture parameters\n",
        "3. Model compilation settings\n",
        "4. Training parameters\n",
        "5. Text generation temperature\n",
        "\n",
        "The model will learn to generate text character-by-character in Shakespeare's style!\n"
      ],
      "metadata": {
        "id": "qKhSmeP0p0nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import random\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import sys\n",
        "import re\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "id": "kZU4Dy49qFDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 1: DOWNLOAD AND LOAD THE CORPUS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 1: Downloading Shakespeare Corpus\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def download_shakespeare():\n",
        "    \"\"\"\n",
        "    Download Shakespeare's complete works from Project Gutenberg.\n",
        "    This is a public domain text perfect for text generation exercises.\n",
        "    \"\"\"\n",
        "    print(\"Downloading Shakespeare's complete works...\")\n",
        "\n",
        "    # URL to Shakespeare's complete works (public domain)\n",
        "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        text = response.text\n",
        "        print(f\"âœ“ Download successful!\")\n",
        "        print(f\"âœ“ Corpus size: {len(text):,} characters\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading: {e}\")\n",
        "        print(\"Using a fallback sample text...\")\n",
        "        # Fallback sample if download fails\n",
        "        return \"\"\"To be, or not to be, that is the question:\n",
        "Whether 'tis nobler in the mind to suffer\n",
        "The slings and arrows of outrageous fortune,\n",
        "Or to take arms against a sea of troubles\n",
        "And by opposing end them. To dieâ€”to sleep,\n",
        "No more; and by a sleep to say we end\n",
        "The heart-ache and the thousand natural shocks\n",
        "That flesh is heir to: 'tis a consummation\n",
        "Devoutly to be wish'd.\"\"\" * 100\n",
        "\n",
        "# Download the corpus\n",
        "raw_text = download_shakespeare()\n",
        "\n",
        "# Show a preview\n",
        "print(\"\\nCorpus preview (first 500 characters):\")\n",
        "print(\"-\" * 70)\n",
        "print(raw_text[:500])\n",
        "print(\"-\" * 70)"
      ],
      "metadata": {
        "id": "fhzFK_sFqGeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 2: PREPROCESS THE TEXT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 2: Preprocessing Text Data\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Convert to lowercase for simplicity (optional - can keep original case)\n",
        "text = raw_text.lower()\n",
        "# text = text.replace('\\n', ' ')    # Remove newlines\n",
        "# text = text.replace('\\r', ' ')    # Remove carriage returns\n",
        "# text = text.replace('\\t', ' ')    # Remove tabs\n",
        "# text = re.sub(r' +', ' ', text)  # Collapse multiple spaces\n",
        "\n",
        "# Get all unique characters in the text\n",
        "chars = sorted(list(set(text)))\n",
        "n_chars = len(chars)\n",
        "\n",
        "print(f\"Total characters in corpus: {len(text):,}\")\n",
        "print(f\"Unique characters: {n_chars}\")\n",
        "print(f\"Characters: {chars[:50]}...\")  # Show first 50 characters\n",
        "\n",
        "# Create character-to-integer mappings\n",
        "char_to_int = {c: i for i, c in enumerate(chars)}\n",
        "int_to_char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "print(f\"\\nExample mappings:\")\n",
        "print(f\"  '{chars[0]}' -> {char_to_int[chars[0]]}\")\n",
        "print(f\"  '{chars[1]}' -> {char_to_int[chars[1]]}\")\n",
        "print(f\"  {char_to_int[chars[0]]} -> '{int_to_char[char_to_int[chars[0]]]}'\")"
      ],
      "metadata": {
        "id": "jVbnxLpzqJb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 3: CREATE TRAINING SEQUENCES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 3: Creating Training Sequences\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# TODO: Student fills this - Define sequence length\n",
        "# This is how many characters the model looks at to predict the next character\n",
        "# Hint: Try values like 40, 60, or 100\n",
        "# Longer sequences = more context but slower training\n",
        "seq_length = ___________\n",
        "\n",
        "print(f\"Sequence length: {seq_length}\")\n",
        "\n",
        "# Prepare the dataset\n",
        "# We'll create overlapping sequences of length seq_length\n",
        "# For each sequence, the target is the next character\n",
        "\n",
        "X_data = []  # Input sequences (encoded as integers)\n",
        "y_data = []  # Target characters (next character after each sequence)\n",
        "\n",
        "# Prepare the dataset with a step size for better coverage\n",
        "step = 3  # Create overlapping sequences every 3 characters for training data\n",
        "\n",
        "print(\"Creating sequences...\")\n",
        "for i in range(0, len(text) - seq_length, step):\n",
        "    # Input: sequence of characters\n",
        "    seq_in = text[___________]\n",
        "    # Output: next character\n",
        "    seq_out = text[___________]\n",
        "\n",
        "    # Convert characters to integers\n",
        "    X_data.append([char_to_int[char] for char in seq_in])\n",
        "    y_data.append(char_to_int[seq_out])\n",
        "\n",
        "n_patterns = len(X_data)\n",
        "print(f\"âœ“ Total training sequences: {n_patterns:,}\")\n",
        "\n",
        "# Example of what we created\n",
        "print(f\"\\nExample sequence:\")\n",
        "print(f\"  Input:  '{text[0:seq_length]}'\")\n",
        "print(f\"  Target: '{text[seq_length]}'\")\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(X_data)\n",
        "y = np.array(y_data)\n",
        "\n",
        "print(f\"\\nData shapes before normalization:\")\n",
        "print(f\"  X shape: {X.shape} (samples, sequence_length)\")\n",
        "print(f\"  y shape: {y.shape} (samples,)\")"
      ],
      "metadata": {
        "id": "zeiS8yZWqNu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 4: NORMALIZE AND RESHAPE DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 4: Normalizing and Reshaping Data\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Normalize input to 0-1 range\n",
        "# This helps with training stability\n",
        "X = X / float(n_chars)\n",
        "\n",
        "# Reshape X to be [samples, time steps, features]\n",
        "# LSTM expects 3D input: (batch_size, sequence_length, input_dim)\n",
        "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
        "\n",
        "# Convert target to one-hot encoding\n",
        "# Each character becomes a binary vector of length n_chars\n",
        "y = to_categorical(y, num_classes=n_chars)\n",
        "\n",
        "print(f\"Data shapes after processing:\")\n",
        "print(f\"  X shape: {X.shape} (samples, sequence_length, features)\")\n",
        "print(f\"  y shape: {y.shape} (samples, num_classes)\")\n",
        "print(f\"\\nThe model will predict a probability distribution over {n_chars} possible characters\")"
      ],
      "metadata": {
        "id": "ZNmlN8p_qTWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 5: BUILD THE LSTM MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 5: Building Character-Level LSTM Model\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def build_lstm_model():\n",
        "    \"\"\"\n",
        "    Build an LSTM model for character-level text generation.\n",
        "\n",
        "    Architecture:\n",
        "    1. Input layer: Defines input shape\n",
        "    2. LSTM layer(s): Learn sequential patterns in text\n",
        "    3. Dropout layer(s): Prevent overfitting\n",
        "    4. Dense output layer: Predict next character\n",
        "\n",
        "    The model learns to predict the probability distribution over all\n",
        "    possible characters given a sequence of previous characters.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        # Input layer: shape is (seq_length, 1)\n",
        "        # 1 feature because we feed one character at a time\n",
        "        Input(shape=(seq_length, 1)),\n",
        "\n",
        "        # TODO: Student fills this - Add first LSTM layer\n",
        "        # Hint: Use 128 or 256 units for good performance\n",
        "        # Set return_sequences=True if you want to stack another LSTM layer\n",
        "        # Set return_sequences=False if this is the last LSTM layer\n",
        "        LSTM(units=___________, return_sequences=___________),\n",
        "\n",
        "        # Dropout to prevent overfitting\n",
        "        Dropout(0.2),\n",
        "\n",
        "        # TODO: Student fills this (OPTIONAL) - Add second LSTM layer\n",
        "        # Hint: If you added return_sequences=True above, uncomment this\n",
        "        # Use same or fewer units (128 or 256)\n",
        "        # This layer should have return_sequences=False\n",
        "        # LSTM(units=___________, return_sequences=False),\n",
        "        # Dropout(0.2),\n",
        "\n",
        "        # TODO: Student fills this - Add Dense output layer\n",
        "        # Hint: Output size should be n_chars (number of unique characters)\n",
        "        # What activation function gives us a probability distribution?\n",
        "        Dense(n_chars, activation='___________')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build the model\n",
        "model = build_lstm_model()\n",
        "\n",
        "# Display model architecture\n",
        "print(\"\\nLSTM Text Generation Model:\")\n",
        "model.summary()\n",
        "\n",
        "print(f\"\\nModel insights:\")\n",
        "print(f\"  - Input: sequences of {seq_length} characters\")\n",
        "print(f\"  - Output: probability distribution over {n_chars} characters\")\n",
        "print(f\"  - Training will teach the model Shakespeare's writing patterns\")"
      ],
      "metadata": {
        "id": "drk-WOOLqVKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 6: COMPILE THE MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 6: Compiling Model\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# TODO: Student fills these - Specify loss and optimizer\n",
        "# Hints:\n",
        "# - Loss: We're doing multi-class classification (use 'categorical_crossentropy')\n",
        "# - Optimizer: 'adam' works well, or try 'rmsprop'\n",
        "\n",
        "loss_function = '___________'\n",
        "optimizer_name = '___________'\n",
        "\n",
        "print(f\"Loss function: {loss_function}\")\n",
        "print(f\"Optimizer: {optimizer_name}\")\n",
        "\n",
        "# IMPORTANT: Gradient clipping prevents exploding gradients in RNNs\n",
        "# This is crucial for stable training!\n",
        "if optimizer_name.lower() == 'adam':\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    optimizer = Adam(learning_rate=0.002, clipnorm=1.0)\n",
        "    print(\"Using Adam optimizer with gradient clipping (clipnorm=1.0)\")\n",
        "elif optimizer_name.lower() == 'rmsprop':\n",
        "    from tensorflow.keras.optimizers import RMSprop\n",
        "    optimizer = RMSprop(learning_rate=0.002, clipnorm=1.0)\n",
        "    print(\"Using RMSprop optimizer with gradient clipping (clipnorm=1.0)\")\n",
        "else:\n",
        "    optimizer = optimizer_name\n",
        "    print(\"Note: Consider using gradient clipping for stable RNN training!\")\n",
        "\n",
        "model.compile(\n",
        "    loss=loss_function,\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"âœ“ Model compiled successfully\")\n",
        "print(\"\\nðŸ’¡ Key Training Settings:\")\n",
        "print(\"  - Gradient clipping enabled (clipnorm=1.0)\")\n",
        "print(\"  - This prevents exploding gradients common in RNNs\")\n",
        "print(\"  - Learning rate: 0.001 (will be reduced automatically if needed)\")"
      ],
      "metadata": {
        "id": "m93XMNgQqync"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 7: SET UP CALLBACKS (Including Text Generation During Training)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 7: Setting Up Training Callbacks\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Custom callback to generate text at specific epochs\n",
        "class TextGenerationCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Custom callback to generate text during training at specific epochs.\n",
        "    This lets us see how generation quality improves over time!\n",
        "    \"\"\"\n",
        "    def __init__(self, seed_text, generate_at_epochs=[1, 2, 5, 10, 15, 20]):\n",
        "        super().__init__()\n",
        "        self.seed_text = seed_text\n",
        "        self.generate_at_epochs = generate_at_epochs\n",
        "        self.generation_history = {}\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \"\"\"Generate text after specific epochs to show progress\"\"\"\n",
        "        current_epoch = epoch + 1  # Keras uses 0-indexing\n",
        "\n",
        "        if current_epoch in self.generate_at_epochs:\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"ðŸŽ­ Text Generation Preview at Epoch {current_epoch}\")\n",
        "            print(f\"{'='*70}\")\n",
        "\n",
        "            # Prepare seed\n",
        "            seed = self.seed_text[-seq_length:] if len(self.seed_text) >= seq_length else self.seed_text\n",
        "            generated = seed\n",
        "\n",
        "            # Generate 200 characters\n",
        "            for i in range(200):\n",
        "                x_pred = np.zeros((1, seq_length, 1))\n",
        "                for t, char in enumerate(generated[-seq_length:]):\n",
        "                    if char in char_to_int:\n",
        "                        x_pred[0, t, 0] = char_to_int[char] / float(n_chars)\n",
        "\n",
        "                predictions = self.model.predict(x_pred, verbose=0)[0]\n",
        "\n",
        "                # Use temperature = 0.2 for more determinstic generation\n",
        "                predictions = np.asarray(predictions).astype('float64')\n",
        "                predictions = np.log(predictions + 1e-10) / 0.2\n",
        "                exp_preds = np.exp(predictions)\n",
        "                predictions = exp_preds / np.sum(exp_preds)\n",
        "                next_index = np.argmax(np.random.multinomial(1, predictions, 1))\n",
        "\n",
        "                next_char = int_to_char[next_index]\n",
        "                generated += next_char\n",
        "\n",
        "            # Store and display\n",
        "            self.generation_history[current_epoch] = generated\n",
        "            print(f\"Seed: '{seed}'\")\n",
        "            print(f\"\\nGenerated text:\")\n",
        "            print(\"-\" * 70)\n",
        "            print(generated)\n",
        "            print(\"-\" * 70)\n",
        "\n",
        "# Initialize callbacks\n",
        "print(\"Setting up training callbacks...\")\n",
        "\n",
        "# Choose a seed text for generation during training\n",
        "generation_seed = text[1000:1000+seq_length]\n",
        "\n",
        "# Text generation callback - shows progress during training!\n",
        "text_gen_callback = TextGenerationCallback(\n",
        "    seed_text=generation_seed,\n",
        "    generate_at_epochs=[1, 2, 5, 10, 15, 20]\n",
        ")\n",
        "\n",
        "# Checkpoint: Save the best model during training\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_model.keras',\n",
        "    monitor='loss',\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "# Reduce learning rate when loss plateaus\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='loss',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        "    min_lr=0.000001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "callbacks = [text_gen_callback, checkpoint, reduce_lr]\n",
        "print(\"âœ“ Callbacks configured:\")\n",
        "print(\"  - Text generation: Shows progress at epochs 1, 2, 5, 10, 15, 20\")\n",
        "print(\"  - Model checkpoint: Saves best model\")\n",
        "print(\"  - Learning rate reduction: Adapts learning rate if loss plateaus\")\n",
        "print(f\"\\nðŸ“ Will generate preview text at epochs: {text_gen_callback.generate_at_epochs}\")\n",
        "print(\"\\nâš ï¸  Training Tips:\")\n",
        "print(\"  - If loss increases, training will automatically reduce learning rate\")\n",
        "print(\"  - Gradient clipping (clipnorm=1.0) prevents exploding gradients\")\n",
        "print(\"  - Loss should steadily decrease; if it doesn't, stop and check settings\")"
      ],
      "metadata": {
        "id": "UVRwpAIFqzq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 8: TRAIN THE MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 8: Training the Model\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# TODO: Student fills these - Set training hyperparameters\n",
        "# Hints:\n",
        "# - batch_size: Try 128, 256, or 512 (larger is faster but uses more memory)\n",
        "# - epochs: Text generation needs more epochs, try 20-50\n",
        "#   (Note: We'll use fewer for the exercise to save time)\n",
        "\n",
        "batch_size = ___________\n",
        "epochs = ___________\n",
        "\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Epochs: {epochs}\")\n",
        "print(f\"Total training samples: {len(X):,}\")\n",
        "print(f\"Steps per epoch: {len(X) // batch_size}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"Starting training... (This may take a while)\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "history = model.fit(\n",
        "    X, y,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nâœ“ Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
        "print(f\"âœ“ Final loss: {history.history['loss'][-1]:.4f}\")\n",
        "print(f\"âœ“ Final accuracy: {history.history['accuracy'][-1]:.4f}\")"
      ],
      "metadata": {
        "id": "n9MnyaYwq7Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 8A: REVIEW GENERATION PROGRESS DURING TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 8A: Reviewing Text Generation Progress\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nLet's see how text generation quality improved during training!\\n\")\n",
        "\n",
        "# Save generation history to file\n",
        "with open('generation_progress.txt', 'w') as f:\n",
        "    f.write(\"Text Generation Progress During Training\\n\")\n",
        "    f.write(\"=\" * 70 + \"\\n\")\n",
        "    f.write(f\"Seed text: '{generation_seed}'\\n\")\n",
        "    f.write(\"Temperature: 1.0 (for consistent comparison)\\n\")\n",
        "    f.write(\"=\" * 70 + \"\\n\\n\")\n",
        "\n",
        "    for epoch_num in sorted(text_gen_callback.generation_history.keys()):\n",
        "        generated_text = text_gen_callback.generation_history[epoch_num]\n",
        "        f.write(f\"\\n{'='*70}\\n\")\n",
        "        f.write(f\"EPOCH {epoch_num}\\n\")\n",
        "        f.write(f\"{'='*70}\\n\")\n",
        "        f.write(generated_text + \"\\n\")\n",
        "\n",
        "print(\"âœ“ Generation progress saved to 'generation_progress.txt'\")\n",
        "\n",
        "# Display a nice comparison\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERATION QUALITY COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for epoch_num in sorted(text_gen_callback.generation_history.keys()):\n",
        "    generated_text = text_gen_callback.generation_history[epoch_num]\n",
        "    # Show first 150 characters\n",
        "    preview = generated_text[:150] + \"...\" if len(generated_text) > 150 else generated_text\n",
        "\n",
        "    print(f\"\\nðŸ“… Epoch {epoch_num}:\")\n",
        "    print(\"-\" * 70)\n",
        "    print(preview)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"OBSERVATION: Notice how the text becomes more coherent as training progresses!\")\n",
        "print(\"Early epochs: Random characters or repeated patterns\")\n",
        "print(\"Later epochs: Recognizable words, better grammar, Shakespeare-like style\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "e56gWZZlrA1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 9: VISUALIZE TRAINING HISTORY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 9: Visualizing Training Progress\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Loss over epochs\n",
        "axes[0].plot(history.history['loss'], marker='o', linewidth=2)\n",
        "axes[0].set_title('Model Loss During Training', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss (Categorical Cross-Entropy)')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Accuracy over epochs\n",
        "axes[1].plot(history.history['accuracy'], marker='o', linewidth=2, color='green')\n",
        "axes[1].set_title('Model Accuracy During Training', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
        "print(\"âœ“ Training history plot saved\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nm9ECMfurRfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 10: TEXT GENERATION FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 10: Defining Text Generation Functions\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def sample_with_temperature(predictions, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Sample a character from a probability distribution with temperature.\n",
        "\n",
        "    Temperature controls randomness:\n",
        "    - temperature = 1.0: Use the model's predicted probabilities as-is\n",
        "    - temperature < 1.0: More conservative (picks high probability chars)\n",
        "    - temperature > 1.0: More creative/random (explores low probability chars)\n",
        "\n",
        "    Args:\n",
        "        predictions: Array of probabilities for each character\n",
        "        temperature: Controls randomness (default 1.0)\n",
        "\n",
        "    Returns:\n",
        "        Index of the sampled character\n",
        "    \"\"\"\n",
        "    predictions = np.asarray(predictions).astype('float64')\n",
        "\n",
        "    # Apply temperature\n",
        "    predictions = np.log(predictions + 1e-10) / temperature\n",
        "    exp_preds = np.exp(predictions)\n",
        "    predictions = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "    # Sample from the distribution\n",
        "    probas = np.random.___________(1, predictions, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def generate_text(model, seed_text, length=400, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generate text using the trained model.\n",
        "\n",
        "    Args:\n",
        "        model: Trained LSTM model\n",
        "        seed_text: Starting text (should be at least seq_length chars)\n",
        "        length: Number of characters to generate\n",
        "        temperature: Sampling temperature (controls creativity)\n",
        "\n",
        "    Returns:\n",
        "        Generated text string\n",
        "    \"\"\"\n",
        "    # Ensure seed text is long enough\n",
        "    if len(seed_text) < seq_length:\n",
        "        seed_text = seed_text + ' ' * (seq_length - len(seed_text))\n",
        "\n",
        "    # Use only the last seq_length characters\n",
        "    seed_text = seed_text[-seq_length:]\n",
        "    generated = seed_text\n",
        "\n",
        "    print(f\"Generating {length} characters with temperature={temperature}...\")\n",
        "    print(f\"Seed text: '{seed_text}'\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # Generate characters one by one\n",
        "    for i in range(length):\n",
        "        # Prepare input sequence\n",
        "        x_pred = np.zeros((1, seq_length, 1))\n",
        "        for t, char in enumerate(generated[-seq_length:]):\n",
        "            if char in char_to_int:\n",
        "                x_pred[0, t, 0] = char_to_int[char] / float(n_chars)\n",
        "\n",
        "        # Predict next character\n",
        "        predictions = model.predict(x_pred, verbose=0)[0]\n",
        "\n",
        "        # Sample next character using temperature\n",
        "        next_index = sample_with_temperature(predictions, temperature)\n",
        "        next_char = int_to_char[next_index]\n",
        "\n",
        "        # Add to generated text\n",
        "        generated += next_char\n",
        "\n",
        "        # Show progress every 100 characters\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Generated {i + 1}/{length} characters...\")\n",
        "\n",
        "    return generated\n",
        "\n",
        "print(\"âœ“ Text generation functions defined\")\n"
      ],
      "metadata": {
        "id": "oR4M9ev9rTrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 11: GENERATE TEXT WITH DIFFERENT TEMPERATURES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 11: Generating Shakespeare-Style Text\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Choose a seed text from the corpus\n",
        "seed_text = text[1000:1000+seq_length]\n",
        "\n",
        "# TODO: Student fills this - Set temperatures for generation\n",
        "# Hint: Try different values to see the effect\n",
        "# - Low temperature (0.2-0.5): Conservative, repetitive\n",
        "# - Medium temperature (0.7-1.0): Balanced\n",
        "# - High temperature (1.2-1.5): Creative, chaotic\n",
        "\n",
        "temperatures = [___________, ___________, ___________]\n",
        "\n",
        "print(f\"Seed text: '{seed_text}'\")\n",
        "print(f\"\\nWe'll generate text at different 'temperatures' to see the effect:\")\n",
        "print(\"  - Low temp: More conservative, follows patterns closely\")\n",
        "print(\"  - High temp: More creative, takes more risks\")\n",
        "\n",
        "# Generate text with different temperatures\n",
        "generated_texts = {}\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Temperature: {temp}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    generated = generate_text(model, seed_text, length=400, temperature=temp)\n",
        "    generated_texts[temp] = generated\n",
        "\n",
        "    print(\"\\nGenerated text:\")\n",
        "    print(\"-\" * 70)\n",
        "    print(generated)\n",
        "    print(\"-\" * 70)"
      ],
      "metadata": {
        "id": "NGNsMPbCrj7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 12: SAVE GENERATED TEXTS TO FILE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 12: Saving Generated Texts\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "with open('generated_text.txt', 'w') as f:\n",
        "    f.write(\"Shakespeare-Style Text Generation Results\\n\")\n",
        "    f.write(\"=\" * 70 + \"\\n\\n\")\n",
        "    f.write(f\"Training Details:\\n\")\n",
        "    f.write(f\"  - Corpus size: {len(text):,} characters\\n\")\n",
        "    f.write(f\"  - Vocabulary size: {n_chars} unique characters\\n\")\n",
        "    f.write(f\"  - Sequence length: {seq_length}\\n\")\n",
        "    f.write(f\"  - Training time: {training_time:.2f} seconds\\n\")\n",
        "    f.write(f\"  - Final loss: {history.history['loss'][-1]:.4f}\\n\")\n",
        "    f.write(f\"  - Final accuracy: {history.history['accuracy'][-1]:.4f}\\n\\n\")\n",
        "\n",
        "    for temp in temperatures:\n",
        "        f.write(\"=\" * 70 + \"\\n\")\n",
        "        f.write(f\"Temperature: {temp}\\n\")\n",
        "        f.write(\"=\" * 70 + \"\\n\")\n",
        "        f.write(generated_texts[temp] + \"\\n\\n\")\n",
        "\n",
        "print(\"âœ“ Generated texts saved to 'generated_text.txt'\")"
      ],
      "metadata": {
        "id": "emwws7_hrxUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 13: SUMMARY AND KEY TAKEAWAYS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 13: Summary and Key Takeaways\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\"\"\n",
        "TRAINING SUMMARY:\n",
        "{'='*70}\n",
        "Corpus: Shakespeare's complete works\n",
        "Total characters: {len(text):,}\n",
        "Unique characters: {n_chars}\n",
        "Sequence length: {seq_length}\n",
        "Training samples: {len(X):,}\n",
        "\n",
        "Model architecture: LSTM with {model.count_params():,} parameters\n",
        "Training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\n",
        "Final loss: {history.history['loss'][-1]:.4f}\n",
        "Final accuracy: {history.history['accuracy'][-1]:.4f}\n",
        "\n",
        "KEY CONCEPTS LEARNED:\n",
        "{'='*70}\n",
        "1. CHARACTER-LEVEL MODELING:\n",
        "   - Models text as a sequence of characters (not words)\n",
        "   - Learns spelling, punctuation, and style patterns\n",
        "   - Can generate novel words and names\n",
        "\n",
        "2. SEQUENCE PREDICTION:\n",
        "   - Uses previous characters to predict the next one\n",
        "   - Sequence length determines how much context the model sees\n",
        "   - Longer sequences = more context but slower training\n",
        "\n",
        "3. ONE-HOT ENCODING:\n",
        "   - Each character converted to a binary vector\n",
        "   - Output is a probability distribution over all characters\n",
        "   - Categorical cross-entropy loss for multi-class prediction\n",
        "\n",
        "4. LSTM FOR TEXT:\n",
        "   - LSTM remembers long-term patterns in text\n",
        "   - Learns grammar, style, and structure\n",
        "   - Can generate coherent text in the training style\n",
        "\n",
        "5. TEMPERATURE SAMPLING:\n",
        "   - Controls randomness in generation\n",
        "   - Low temp: Safe, predictable, repetitive\n",
        "   - High temp: Creative, diverse, sometimes nonsensical\n",
        "   - Balances between mimicry and creativity\n",
        "\n",
        "APPLICATIONS:\n",
        "{'='*70}\n",
        "- Creative writing assistance\n",
        "- Code generation\n",
        "- Music composition (with notes as \"characters\")\n",
        "- DNA sequence analysis\n",
        "- Language modeling\n",
        "- Auto-completion systems\n",
        "\n",
        "NEXT STEPS:\n",
        "{'='*70}\n",
        "1. Try longer training (more epochs)\n",
        "2. Experiment with different sequence lengths\n",
        "3. Use word-level modeling instead of character-level\n",
        "4. Try different texts (modern novels, code, poetry)\n",
        "5. Implement beam search for better generation\n",
        "6. Add attention mechanisms\n",
        "7. Try transformer models (GPT-style)\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Exercise completed! Review your generated text and discuss with your instructor.\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "mZRgC5SHr26s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The KEY Insight:\n",
        "\n",
        "### 60% accuracy = 60% of INDIVIDUAL characters correct\n",
        "But over 200 characters:\n",
        "\n",
        "- âœ… ~120 correct characters\n",
        "- âŒ ~80 wrong characters\n",
        "- These 80 errors compound and break the flow!\n",
        "\n",
        "Plus, with temperature=1.0:\n",
        "\n",
        "- Model picks from FULL probability distribution\n",
        "- 40% of picks are suboptimal\n",
        "- Makes text look MORE random than it actually is\n",
        "\n",
        "\n",
        "### ðŸŽ­ What Your Model Learned vs Didn't Learn\n",
        "âœ… At 60%, Your Model HAS Learned:\n",
        "\n",
        "- Common words (the, and, be, to, not)\n",
        "- Basic grammar\n",
        "- Letter patterns (th, er, ing)\n",
        "- Word boundaries\n",
        "\n",
        "âŒ What It HASN'T Learned Yet:\n",
        "\n",
        "- Shakespeare's archaic language (thou, 'tis, wherefore)\n",
        "- Complex sentence structures\n",
        "- Poetic rhythm\n",
        "- Long-range coherence\n",
        "\n",
        "### Why not? Needs 70%+ accuracy for style!\n",
        "\n",
        "ðŸš€ How to Fix It (Ranked by Impact)\n",
        "1. Train Much Longer ðŸ“ˆ (BIGGEST IMPACT)\n",
        "- epochs = 100  # Instead of 20\n",
        "- Result: 58% â†’ 66% accuracy, Shakespeare style emerges!\n",
        "2. Lower Temperature âš¡ (IMMEDIATE FIX)\n",
        "- generate_text(model, seed, temperature=0.6)  # Instead of 1.0\n",
        "- Result: 15-20% better quality instantly!\n",
        "3. Remove Line Breaks\n",
        "- Result: Cleaner, faster learning\n",
        "4. Larger Model (Optional)\n",
        "- LSTM(units=512, ...)  # Instead of 256\n",
        "- Result: More capacity for complex patterns"
      ],
      "metadata": {
        "id": "MSp_tZWBA53I"
      }
    }
  ]
}